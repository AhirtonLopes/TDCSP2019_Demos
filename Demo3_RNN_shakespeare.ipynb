{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Demo3_RNN_shakespeare.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AhirtonLopes/TDCSP2019_Demos/blob/master/Demo3_RNN_shakespeare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuauS__EBg-o",
        "colab_type": "text"
      },
      "source": [
        "# Demo 3 - Gerador automatizado de textos no estilo de William Shakespeare\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbUyEL-EEGn7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importando bibliotecas utilizadas\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.cuda as cuda\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1476pwvEJjX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Baixando nossa base de dados a ser utilizada na demo\n",
        "\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdMOTSxTEL4i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Declarando nosso corpus textual a ser utilizado\n",
        "\n",
        "corpus = open(path_to_file, 'rb').read().decode(encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NO-F8VhJEU2q",
        "colab_type": "code",
        "outputId": "82ef1c31-6959-48a1-a731-fdb266145a6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "print(\"Numero de palavras no corpus textual: \",len(corpus))\n",
        "text=corpus.splitlines()\n",
        "print(\"Numero de linhas no corpus textual \",len(text))\n",
        "\n",
        "print(\"A primeira linha do corpus textual é => \",text[0])\n",
        "\n",
        "# Criando nossos conjuntos de treino, teste e validação\n",
        "\n",
        "for i in range(len(text)):\n",
        "  text[i]=text[i]+\" <end>\" ## adicionando <end> ao final de cada linha \n",
        "print(\"Agora a primeira linha do corpus textual é => \",text[0]) \n",
        "\n",
        "\n",
        "words=' '.join(text)\n",
        "words=words.split()\n",
        "print(\"Mostrando as palavras de posição 0 a 100 em nosso corpus : \",words[0:100])\n",
        "\n",
        "# Usamos essas palavras para criar vocabulário"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Numero de palavras no corpus textual:  1115394\n",
            "Numero de linhas no corpus textual  40000\n",
            "A primeira linha do corpus textual é =>  First Citizen:\n",
            "Agora a primeira linha do corpus textual é =>  First Citizen: <end>\n",
            "Mostrando as palavras de posição 0 a 100 em nosso corpus :  ['First', 'Citizen:', '<end>', 'Before', 'we', 'proceed', 'any', 'further,', 'hear', 'me', 'speak.', '<end>', '<end>', 'All:', '<end>', 'Speak,', 'speak.', '<end>', '<end>', 'First', 'Citizen:', '<end>', 'You', 'are', 'all', 'resolved', 'rather', 'to', 'die', 'than', 'to', 'famish?', '<end>', '<end>', 'All:', '<end>', 'Resolved.', 'resolved.', '<end>', '<end>', 'First', 'Citizen:', '<end>', 'First,', 'you', 'know', 'Caius', 'Marcius', 'is', 'chief', 'enemy', 'to', 'the', 'people.', '<end>', '<end>', 'All:', '<end>', 'We', \"know't,\", 'we', \"know't.\", '<end>', '<end>', 'First', 'Citizen:', '<end>', 'Let', 'us', 'kill', 'him,', 'and', \"we'll\", 'have', 'corn', 'at', 'our', 'own', 'price.', '<end>', \"Is't\", 'a', 'verdict?', '<end>', '<end>', 'All:', '<end>', 'No', 'more', 'talking', \"on't;\", 'let', 'it', 'be', 'done:', 'away,', 'away!', '<end>', '<end>', 'Second']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ga3Wtuq0xMx",
        "colab_type": "code",
        "outputId": "4a9991cd-3ac5-44af-d556-c56b39f253e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Criando vocabulário\n",
        "\n",
        "voc=[]\n",
        "\n",
        "def return_index(word):\n",
        "  return voc.index(word)\n",
        " \n",
        "for word in words :\n",
        "  if word not in voc:\n",
        "    voc.append(word)\n",
        "    \n",
        "print(\"Indice da palavra (The) no vocabulario: \",return_index('The') )\n",
        "print(\"A palavra para o indice de numero (203): \",voc[203] )\n",
        "\n",
        "print(\"Comprimento de nosso vocabulario: \",len(voc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Indice da palavra (The) no vocabulario:  203\n",
            "A palavra para o indice de numero (203):  The\n",
            "Comprimento de nosso vocabulario:  25671\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJxYsGfx7OIT",
        "colab_type": "code",
        "outputId": "a2f978b1-b619-47a9-c5d3-325313b8d74e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Convertendo palavras em índices e salvando em words_indexes\n",
        "\n",
        "words_indexes=[return_index(word) for word in words]\n",
        "print(\"Imprimindo alguns de nossos primeiros indices : \",words_indexes[0:100])\n",
        "\n",
        "# Utilizamos words_indexes como entrada para nossa rede\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Imprimindo alguns de nossos primeiros indices :  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 2, 11, 2, 12, 10, 2, 2, 0, 1, 2, 13, 14, 15, 16, 17, 18, 19, 20, 18, 21, 2, 2, 11, 2, 22, 23, 2, 2, 0, 1, 2, 24, 25, 26, 27, 28, 29, 30, 31, 18, 32, 33, 2, 2, 11, 2, 34, 35, 4, 36, 2, 2, 0, 1, 2, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 2, 49, 50, 51, 2, 2, 11, 2, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 2, 2, 62]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cjjZtu9-0o0",
        "colab_type": "code",
        "outputId": "51624015-7d55-48f9-8448-93d6a6a0861e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "seq_lenght=35 # comprimento de nossa RNN (comprimento do input)\n",
        "batch_size=20\n",
        "# quantos batches(com tamanho: [batch_size, seq_lenght])  temos para nosso array words_indexes ?\n",
        "# resposta => len(words_indexes/(batch_size*seq_lenght))\n",
        "max_batches=int(len(words_indexes)/(batch_size*seq_lenght))\n",
        "print(\"O numero maximo de batches com essa configuracao e (batch_size,seq_lenght) :  \",max_batches)\n",
        "\n",
        "trainsetX=words_indexes[0:int(0.9*max_batches)*batch_size*seq_lenght]\n",
        "trainsetY=words_indexes[1:int(0.9*max_batches)*batch_size*seq_lenght+1]\n",
        "validsetX=words_indexes[int(0.9*max_batches)*batch_size*seq_lenght:int(1*max_batches)*batch_size*seq_lenght]\n",
        "validsetY=words_indexes[int(0.9*max_batches)*batch_size*seq_lenght + 1:int(1*max_batches)*batch_size*seq_lenght +1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "O numero maximo de batches com essa configuracao e (batch_size,seq_lenght) :   346\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJqJCYHp0x_0",
        "colab_type": "code",
        "outputId": "550e280e-4527-4d45-ba5a-ebb80b81e1a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Criando nossos batches\n",
        "\n",
        "def create_batch(data,flag):\n",
        "  if flag=='Train':\n",
        "     return (np.array(data).reshape(int(0.9*max_batches) ,batch_size ,seq_lenght)) # shape (número do batch, batch_size,seq_enght)\n",
        "  else :\n",
        "     return (np.array(data).reshape(int(1*max_batches)-int(0.9*max_batches) ,batch_size ,seq_lenght)) # shape (número do batch, batch_size,seq_enght)\n",
        "  \n",
        "trainX_batches =create_batch(trainsetX,'Train')\n",
        "trainY_batches =create_batch(trainsetY,'Train')\n",
        "\n",
        "validX_batches =create_batch(validsetX,'Valid')\n",
        "validY_batches =create_batch(validsetY,'Valid')\n",
        "\n",
        "\n",
        "print(\"trainX_batches_shape: \",trainX_batches.shape)\n",
        "print(\"trainY_batches_shape: \",trainY_batches.shape)\n",
        "\n",
        "print(\"validX_batches_shape: \",validX_batches.shape)\n",
        "print(\"validY_batches_shape: \",validY_batches.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trainX_batches_shape:  (311, 20, 35)\n",
            "trainY_batches_shape:  (311, 20, 35)\n",
            "validX_batches_shape:  (35, 20, 35)\n",
            "validY_batches_shape:  (35, 20, 35)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaCR9Gi0DEv0",
        "colab_type": "code",
        "outputId": "9e798d60-a3d2-4a0f-8575-1c5fbfa1b671",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print(trainX_batches[0,0,:]) # batchNumber=0 , amostra de número 0 no batch , em todos os comprimentos de sequência\n",
        "print(trainY_batches[0,0,:])\n",
        "print(len(words_indexes))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0  1  2  3  4  5  6  7  8  9 10  2  2 11  2 12 10  2  2  0  1  2 13 14\n",
            " 15 16 17 18 19 20 18 21  2  2 11]\n",
            "[ 1  2  3  4  5  6  7  8  9 10  2  2 11  2 12 10  2  2  0  1  2 13 14 15\n",
            " 16 17 18 19 20 18 21  2  2 11  2]\n",
            "242651\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBT0GZrS1IoH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Agregamos todas as informações em uma função\n",
        "\n",
        "def get_batches():# uso no treinamento\n",
        "  global  words_indexes\n",
        "  a=[]\n",
        "  b=[]\n",
        "  \n",
        "    # words_indexes shuffle\n",
        "  a=words_indexes[0:int(0.2*len(words_indexes))]\n",
        "  b=words_indexes[int(0.2*len(words_indexes)):]\n",
        "  words_indexes=b+a\n",
        "\n",
        "  trainsetX=words_indexes[0:int(0.9*max_batches)*batch_size*seq_lenght]\n",
        "  trainsetY=words_indexes[1:int(0.9*max_batches)*batch_size*seq_lenght+1]\n",
        "  validsetX=words_indexes[int(0.9*max_batches)*batch_size*seq_lenght:int(1*max_batches)*batch_size*seq_lenght]\n",
        "  validsetY=words_indexes[int(0.9*max_batches)*batch_size*seq_lenght + 1:int(1*max_batches)*batch_size*seq_lenght +1]\n",
        "\n",
        "  trainX_batches =create_batch(trainsetX,'Train')\n",
        "  trainY_batches =create_batch(trainsetY,'Train')\n",
        "  validX_batches =create_batch(validsetX,'Valid')\n",
        "  validY_batches =create_batch(validsetY,'Valid')\n",
        "  \n",
        "  return  trainX_batches,trainY_batches,validX_batches,validY_batches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gPNo1yy1I2i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Criando nossa rede RNN\n",
        "\n",
        "class net(torch.nn.Module):\n",
        "  def  __init__(self,vocab_size,embed_size,hidden_size,number_of_layers,num_dir):\n",
        "    super().__init__()\n",
        "    \n",
        "    # Se RNN for biderecional, num_directions será 2, senão 1\n",
        "  \n",
        "    self.num_directions=num_dir\n",
        "    self.number_of_layers=number_of_layers\n",
        "    self.hidden_size=hidden_size\n",
        "    \n",
        "    self.embd= torch.nn.Embedding(vocab_size, embed_size)\n",
        "    self.drop1=torch.nn.Dropout(0.3)\n",
        "    self.gru = torch.nn.GRU(embed_size, hidden_size, number_of_layers, dropout=0.4,bidirectional=False)\n",
        "    self.drop2=torch.nn.Dropout(0.1)\n",
        "    self.fc=torch.nn.Linear(hidden_size,vocab_size)\n",
        "    self.init_weights()\n",
        "\n",
        "  def forward(self,x,hidden):\n",
        "    \n",
        "    #x shape : torch.Size([seq_lenght, batch_size]) contem os índices de palavras de nosso vocabulário\n",
        "    #hidden shape :torch.Size([num_layers * num_directions, batch_size, hidden_size])\n",
        "    #média camada escondida h0\n",
        "    \n",
        "    emb=self.embd(x) \n",
        "    #emb shape : torch.Size([seq_lenght, batch_size, embed_size])\n",
        "    out=self.drop1(emb)\n",
        "    out , hidden =self.gru(emb,hidden) #  out = a camada escondidade mais alto nível para todos os steps |  hidden= contém valores para todas as camads escondidas do último timestep\n",
        "    #out shape :  torch.Size([seq_lenght, batch_size, hidden_size  * num_directions])\n",
        "    #hidden shape : torch.Size([num_layers * num_directions, batch_size, hidden_size])\n",
        "    \n",
        "    \n",
        "    out=self.drop2(out) # it doesnt  change the dimension\n",
        "   \n",
        "    output=out.view(out.size(0)* out.size(1),out.size(2))   \n",
        "    #out shape:  torch.Size([seq_lenght * batch_size, hidden_size  * num_directions])\n",
        "    \n",
        "    \n",
        "    output=self.fc(output)\n",
        "    #output shape:  torch.Size([seq_lenght * batch_size, vocab_size])\n",
        "    \n",
        "    \n",
        "    output=output.view(out.shape[0], out.shape[1],output.shape[1])   \n",
        "    #output shape:  torch.Size([seq_lenght , batch_size, vocab_size])\n",
        "\n",
        "    \n",
        "    # camada escondida é h0 no póximo processo de feed\n",
        "    return output , hidden\n",
        "  def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.embd.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.fill_(0)\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        \n",
        "  def init_hidden(self, batch_size): # inicialização com zeros\n",
        "        weight = next(self.parameters()).data\n",
        "        return torch.autograd.Variable(weight.new(self.number_of_layers, batch_size, self.hidden_size).zero_())\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stvpINrJ1JGU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size=len(voc)\n",
        "embed_size=250\n",
        "hidden_size=200\n",
        "number_of_layers=2\n",
        "num_direction=1\n",
        "learning_rate=0.001\n",
        "clip = 0.25"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AANGMmnTQjw0",
        "colab_type": "code",
        "outputId": "8303cc53-bd0e-4b8c-87ab-114d64fe70b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "model=net(vocab_size,embed_size,hidden_size,number_of_layers,num_direction)\n",
        "print(model)\n",
        "if cuda.is_available():\n",
        "    model.cuda()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "net(\n",
            "  (embd): Embedding(25671, 250)\n",
            "  (drop1): Dropout(p=0.3)\n",
            "  (gru): GRU(250, 200, num_layers=2, dropout=0.4)\n",
            "  (drop2): Dropout(p=0.1)\n",
            "  (fc): Linear(in_features=200, out_features=25671, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTgsiNIyQls0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtQ1I_Kw5HHg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculating accuracy\n",
        "\n",
        "def cal_accuracy(pred_classes,real_classes):#  shape -> both : [number of test samples,1]  ou both : [number of test samples,]\n",
        "  bool_array=(pred_classes==real_classes) # exemplo: bool_array=[True, False, True, False, True]\n",
        "  True_pred_counts=np.count_nonzero(bool_array) # contando o número de verdadeiros em [True, False, True, False, True]\n",
        "  \n",
        "  return True_pred_counts/pred_classes.shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ap8AXWsIV1ad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#important notices:\n",
        "#a.shape=(2,3,4)\n",
        "#a.reshape(6,4) ->  (a000) (a001) (a002) (a003)      ,a021=a[0,2,1]\n",
        "#                   (a010) (a011) (a012) (a013) \n",
        "#                   (a020) (a021) (a022) (a023) \n",
        "#                   (a100) (a101) (a102) (a103) \n",
        "#                   (a110) (a111) (a112) (a113) \n",
        "#                   (a120) (a121) (a122) (a123) \n",
        "#\n",
        "#\n",
        "#a.shape=(3,2,4)\n",
        "#a.reshape(6,4) ->  (a000) (a001) (a002) (a003) \n",
        "#                   (a010) (a011) (a012) (a013) \n",
        "#                   (a100) (a101) (a102) (a103) \n",
        "#                   (a110) (a111) (a112) (a113) \n",
        "#                   (a200) (a201) (a202) (a203) \n",
        "#                   (a210) (a211) (a212) (a213) \n",
        "#\n",
        "#\n",
        "#a.shape=(3,2)\n",
        "#a.reshape(6,1) ->  (a00) \n",
        "#                   (a01)  \n",
        "#                   (a10)                   \n",
        "#                   (a11)  \n",
        "#                   (a20)  \n",
        "#                   (a21) \n",
        "#\n",
        "#\n",
        "#a.shape=(2,3)\n",
        "#a.reshape(6,1) ->  (a00) \n",
        "#                   (a01)  \n",
        "#                   (a02)                   \n",
        "#                   (a10)  \n",
        "#                   (a11)  \n",
        "#                   (a12) \n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a0rmFKiQmXG",
        "colab_type": "code",
        "outputId": "e2244d3f-f0f7-4225-fcb8-a68022500311",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs=300\n",
        "mean_train_acc=[]\n",
        "mean_valid_acc=[]\n",
        "\n",
        "for i in range(num_epochs):\n",
        "  validacc=[]\n",
        "  trainacc=[]\n",
        "  hidden = model.init_hidden(batch_size)\n",
        "  total_loss=0\n",
        "  model.train()\n",
        "  trainX_batches,trainY_batches,_,_=get_batches()\n",
        "  for batchNumber in range(trainX_batches.shape[0]):\n",
        "    \n",
        "    X=torch.autograd.Variable(torch.LongTensor(trainX_batches[batchNumber].T)) \n",
        "    #train_batches[batchNumber].T  shape :nparray ([seq_lenght,batch_size]) \n",
        "    Y=torch.autograd.Variable(torch.LongTensor(trainY_batches[batchNumber].T.reshape(seq_lenght*batch_size)))  \n",
        "    #Y:Torch.szie ([seq_lenght*batch_size]) => seq0batch0  ,  seq0batch1 ,  seq0batch2  , ..... \n",
        "    if cuda.is_available():\n",
        "            X = X.cuda()\n",
        "            Y = Y.cuda()\n",
        "    \n",
        "    hidden = torch.autograd.Variable(hidden)# separando o estado escondido de uma iteração para outra\n",
        "    # essa em específico será usada para o plot de nosso gráfico computacional\n",
        "   \n",
        "    if cuda.is_available():\n",
        "             hidden = hidden.cuda()\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    outputs,hidden=model(X,hidden)# outputs shape:  torch.Size([seq_lenght , batch_size, vocab_size])\n",
        "    outputs=outputs.view(seq_lenght*batch_size, vocab_size)\n",
        "    #outputs shape:  torch.Size([seq_lenght * batch_size, vocab_size])=> seq0batch0,vocab  ,  seq0batch1,vocab ,  seq0batch2,vocab  , .....\n",
        "    \n",
        "    loss=criterion(outputs,Y)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm(model.parameters(), clip)\n",
        "    optimizer.step()\n",
        "\n",
        "    pred=outputs\n",
        "    pred_classes=torch.max(pred, 1)[1]# pred_classes.shape : torch.Size([seq_lenght * batch_size])\n",
        "    acc=cal_accuracy(np.array(pred_classes.cpu()),Y.cpu().numpy()) \n",
        "    trainacc.append(acc*100)\n",
        "\n",
        "  print(\"Acuracia de treinamento na epoca: \",i,\" e \",np.mean(trainacc))\n",
        "  mean_train_acc.append(np.mean(trainacc))\n",
        "  hidden = model.init_hidden(batch_size)\n",
        "  model.eval() \n",
        "  _,_,validX_batches,validY_batches=get_batches()\n",
        "\n",
        "  for batchNumber in range(validX_batches.shape[0]):\n",
        "    \n",
        "    X=torch.autograd.Variable(torch.LongTensor(validX_batches[batchNumber].T)) \n",
        "    #valid_batches[batchNumber].T  shape :nparray ([seq_lenght,batch_size]) \n",
        "    Y=torch.autograd.Variable(torch.LongTensor(validY_batches[batchNumber].T.reshape(seq_lenght*batch_size)))  \n",
        "    #Y:Torch.szie ([seq_lenght*batch_size]) \n",
        "    if cuda.is_available():\n",
        "            X = X.cuda()\n",
        "            Y = Y.cuda()\n",
        "  \n",
        "    if cuda.is_available():\n",
        "            hidden = hidden.cuda()\n",
        "      \n",
        "    outputs,hidden=model(X,hidden)\n",
        "    # outputs shape:  torch.Size([seq_lenght , batch_size, vocab_size])\n",
        "    outputs=outputs.view(seq_lenght*batch_size, vocab_size)\n",
        "    # outputs shape:  torch.Size([seq_lenght * batch_size, vocab_size])\n",
        "    pred=outputs \n",
        "    pred_classes=torch.max(pred.data, 1)[1]# torch.Size([seq_lenght * batch_size])\n",
        "    acc=cal_accuracy(np.array(pred_classes.cpu()),Y.cpu().numpy())\n",
        "    validacc.append(acc*100)\n",
        "\n",
        "  print(\"Acuracia de validacao na epoca: \",i,\" e \",np.mean(validacc))\n",
        "  mean_valid_acc.append(np.mean(validacc))\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:35: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Acuracia de treinamento na epoca:  0  e  16.595314653192467\n",
            "Acuracia de validacao na epoca:  0  e  16.08979591836735\n",
            "Acuracia de treinamento na epoca:  1  e  19.01745521359669\n",
            "Acuracia de validacao na epoca:  1  e  19.371428571428567\n",
            "Acuracia de treinamento na epoca:  2  e  20.869085898024803\n",
            "Acuracia de validacao na epoca:  2  e  21.50612244897959\n",
            "Acuracia de treinamento na epoca:  3  e  22.2907671107028\n",
            "Acuracia de validacao na epoca:  3  e  22.5265306122449\n",
            "Acuracia de treinamento na epoca:  4  e  23.214056040422598\n",
            "Acuracia de validacao na epoca:  4  e  23.75102040816327\n",
            "Acuracia de treinamento na epoca:  5  e  23.779513091410198\n",
            "Acuracia de validacao na epoca:  5  e  22.408163265306126\n",
            "Acuracia de treinamento na epoca:  6  e  24.38217730822232\n",
            "Acuracia de validacao na epoca:  6  e  23.024489795918367\n",
            "Acuracia de treinamento na epoca:  7  e  24.74735875057419\n",
            "Acuracia de validacao na epoca:  7  e  24.02857142857143\n",
            "Acuracia de treinamento na epoca:  8  e  25.32384014699127\n",
            "Acuracia de validacao na epoca:  8  e  24.93061224489796\n",
            "Acuracia de treinamento na epoca:  9  e  26.16582452916858\n",
            "Acuracia de validacao na epoca:  9  e  25.579591836734696\n",
            "Acuracia de treinamento na epoca:  10  e  26.69958658704639\n",
            "Acuracia de validacao na epoca:  10  e  23.591836734693874\n",
            "Acuracia de treinamento na epoca:  11  e  27.40927882406982\n",
            "Acuracia de validacao na epoca:  11  e  24.673469387755105\n",
            "Acuracia de treinamento na epoca:  12  e  28.10105649977033\n",
            "Acuracia de validacao na epoca:  12  e  25.951020408163266\n",
            "Acuracia de treinamento na epoca:  13  e  29.326136885622418\n",
            "Acuracia de validacao na epoca:  13  e  27.31428571428571\n",
            "Acuracia de treinamento na epoca:  14  e  30.152044097381715\n",
            "Acuracia de validacao na epoca:  14  e  28.24081632653061\n",
            "Acuracia de treinamento na epoca:  15  e  31.425815342214058\n",
            "Acuracia de validacao na epoca:  15  e  26.510204081632654\n",
            "Acuracia de treinamento na epoca:  16  e  32.34313275149288\n",
            "Acuracia de validacao na epoca:  16  e  26.595918367346943\n",
            "Acuracia de treinamento na epoca:  17  e  33.35737253100598\n",
            "Acuracia de validacao na epoca:  17  e  28.383673469387755\n",
            "Acuracia de treinamento na epoca:  18  e  34.58612769866789\n",
            "Acuracia de validacao na epoca:  18  e  28.738775510204082\n",
            "Acuracia de treinamento na epoca:  19  e  35.60174552135967\n",
            "Acuracia de validacao na epoca:  19  e  30.151020408163262\n",
            "Acuracia de treinamento na epoca:  20  e  36.73220027560864\n",
            "Acuracia de validacao na epoca:  20  e  28.840816326530614\n",
            "Acuracia de treinamento na epoca:  21  e  37.316949931097845\n",
            "Acuracia de validacao na epoca:  21  e  28.61632653061224\n",
            "Acuracia de treinamento na epoca:  22  e  37.986678915939365\n",
            "Acuracia de validacao na epoca:  22  e  31.187755102040814\n",
            "Acuracia de treinamento na epoca:  23  e  39.165365181442354\n",
            "Acuracia de validacao na epoca:  23  e  31.673469387755105\n",
            "Acuracia de treinamento na epoca:  24  e  39.937069361506666\n",
            "Acuracia de validacao na epoca:  24  e  33.83673469387755\n",
            "Acuracia de treinamento na epoca:  25  e  40.797887000459355\n",
            "Acuracia de validacao na epoca:  25  e  34.40816326530613\n",
            "Acuracia de treinamento na epoca:  26  e  41.551676619200734\n",
            "Acuracia de validacao na epoca:  26  e  31.461224489795917\n",
            "Acuracia de treinamento na epoca:  27  e  41.88148828663298\n",
            "Acuracia de validacao na epoca:  27  e  34.089795918367344\n",
            "Acuracia de treinamento na epoca:  28  e  42.96876435461644\n",
            "Acuracia de validacao na epoca:  28  e  36.42448979591837\n",
            "Acuracia de treinamento na epoca:  29  e  43.63619660082682\n",
            "Acuracia de validacao na epoca:  29  e  36.714285714285715\n",
            "Acuracia de treinamento na epoca:  30  e  44.49563619660083\n",
            "Acuracia de validacao na epoca:  30  e  38.96326530612246\n",
            "Acuracia de treinamento na epoca:  31  e  44.81763895268718\n",
            "Acuracia de validacao na epoca:  31  e  32.926530612244896\n",
            "Acuracia de treinamento na epoca:  32  e  45.18327974276527\n",
            "Acuracia de validacao na epoca:  32  e  37.02448979591837\n",
            "Acuracia de treinamento na epoca:  33  e  46.036747818098306\n",
            "Acuracia de validacao na epoca:  33  e  40.93469387755102\n",
            "Acuracia de treinamento na epoca:  34  e  47.06936150666054\n",
            "Acuracia de validacao na epoca:  34  e  41.27755102040816\n",
            "Acuracia de treinamento na epoca:  35  e  47.533762057877816\n",
            "Acuracia de validacao na epoca:  35  e  42.00816326530613\n",
            "Acuracia de treinamento na epoca:  36  e  48.27009646302251\n",
            "Acuracia de validacao na epoca:  36  e  36.22857142857143\n",
            "Acuracia de treinamento na epoca:  37  e  48.38906752411575\n",
            "Acuracia de validacao na epoca:  37  e  41.7469387755102\n",
            "Acuracia de treinamento na epoca:  38  e  49.109784106568675\n",
            "Acuracia de validacao na epoca:  38  e  45.216326530612236\n",
            "Acuracia de treinamento na epoca:  39  e  50.01240238860817\n",
            "Acuracia de validacao na epoca:  39  e  44.85306122448979\n",
            "Acuracia de treinamento na epoca:  40  e  50.494717501148365\n",
            "Acuracia de validacao na epoca:  40  e  46.96734693877551\n",
            "Acuracia de treinamento na epoca:  41  e  50.72760679834635\n",
            "Acuracia de validacao na epoca:  41  e  38.453061224489794\n",
            "Acuracia de treinamento na epoca:  42  e  51.061093247588424\n",
            "Acuracia de validacao na epoca:  42  e  44.6204081632653\n",
            "Acuracia de treinamento na epoca:  43  e  51.81993569131833\n",
            "Acuracia de validacao na epoca:  43  e  49.95510204081633\n",
            "Acuracia de treinamento na epoca:  44  e  52.52641249425816\n",
            "Acuracia de validacao na epoca:  44  e  47.42040816326531\n",
            "Acuracia de treinamento na epoca:  45  e  52.90537436839687\n",
            "Acuracia de validacao na epoca:  45  e  48.346938775510196\n",
            "Acuracia de treinamento na epoca:  46  e  53.236564079007806\n",
            "Acuracia de validacao na epoca:  46  e  43.13469387755102\n",
            "Acuracia de treinamento na epoca:  47  e  53.299954065227375\n",
            "Acuracia de validacao na epoca:  47  e  49.20816326530612\n",
            "Acuracia de treinamento na epoca:  48  e  53.87276067983464\n",
            "Acuracia de validacao na epoca:  48  e  50.56326530612245\n",
            "Acuracia de treinamento na epoca:  49  e  54.92788240698208\n",
            "Acuracia de validacao na epoca:  49  e  49.02857142857143\n",
            "Acuracia de treinamento na epoca:  50  e  55.1791456132292\n",
            "Acuracia de validacao na epoca:  50  e  50.497959183673466\n",
            "Acuracia de treinamento na epoca:  51  e  55.37436839687644\n",
            "Acuracia de validacao na epoca:  51  e  43.738775510204086\n",
            "Acuracia de treinamento na epoca:  52  e  55.247129076711076\n",
            "Acuracia de validacao na epoca:  52  e  50.08163265306123\n",
            "Acuracia de treinamento na epoca:  53  e  56.237942122186496\n",
            "Acuracia de validacao na epoca:  53  e  54.44489795918368\n",
            "Acuracia de treinamento na epoca:  54  e  57.00826825907212\n",
            "Acuracia de validacao na epoca:  54  e  54.077551020408166\n",
            "Acuracia de treinamento na epoca:  55  e  57.0206706476803\n",
            "Acuracia de validacao na epoca:  55  e  51.15510204081633\n",
            "Acuracia de treinamento na epoca:  56  e  57.20624712907671\n",
            "Acuracia de validacao na epoca:  56  e  46.68571428571428\n",
            "Acuracia de treinamento na epoca:  57  e  57.448782728525494\n",
            "Acuracia de validacao na epoca:  57  e  53.73061224489796\n",
            "Acuracia de treinamento na epoca:  58  e  58.05879650895728\n",
            "Acuracia de validacao na epoca:  58  e  57.22040816326531\n",
            "Acuracia de treinamento na epoca:  59  e  58.69315571887919\n",
            "Acuracia de validacao na epoca:  59  e  56.77959183673469\n",
            "Acuracia de treinamento na epoca:  60  e  58.8098300413413\n",
            "Acuracia de validacao na epoca:  60  e  55.80816326530612\n",
            "Acuracia de treinamento na epoca:  61  e  59.05879650895729\n",
            "Acuracia de validacao na epoca:  61  e  48.17142857142857\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L856xl4UPKUO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot([i+1 for i in range(num_epochs)],mean_valid_acc,label=\"Acuracia de validacao\")\n",
        "plt.plot([i+1 for i in range(num_epochs)],mean_train_acc,label=\"Acuracia de treinamento\")\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel(\"Epocas\")\n",
        "plt.ylabel(\"Acuracia em % \")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nkLwwJWMfTp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Gerando palavras\n",
        "\n",
        "print(\"GERANDO TEXTO FALSO DE WILLIAM SHAKESPEARE: \\n________________________________________________________\")\n",
        "num_words=2000 # número de palavras que gostaríamos de gerar\n",
        "testacc=[]\n",
        "\n",
        "# Qual a primeira palavra palavra do texto (cold start)? Nesse caso será (The)\n",
        "\n",
        "input_word_index=return_index('The')\n",
        "print('The',end=\" \")\n",
        "X=torch.autograd.Variable(torch.LongTensor(np.array([input_word_index]).reshape(1,1)))\n",
        "hidden = model.init_hidden(1)\n",
        "\n",
        "for i in range(num_words):\n",
        "\n",
        "    if cuda.is_available():\n",
        "            X = X.cuda()\n",
        "           \n",
        "  \n",
        "  \n",
        "#     print(hidden.shape)\n",
        "    outputs,hidden=model(X,hidden)# outputs shape:  torch.Size([seq_lenght , batch_size, vocab_size])\n",
        "    outputs=outputs.view(vocab_size)\n",
        "    outExp=outputs.exp()\n",
        "#     print(outputs)\n",
        "    probablistic_output=(outExp)/(outExp.sum())# probablistic_output shape: torch.Size([vocab_size])\n",
        "#     probablistic_output=((outputs)/(0.4)).exp()\n",
        "\n",
        "    word_index = (torch.multinomial(probablistic_output, 1))\n",
        "    X=torch.autograd.Variable((word_index).reshape(1,1))\n",
        "    if(voc[word_index]==\"<end>\"):\n",
        "      print(\"\")\n",
        "    else:\n",
        "      print(voc[word_index],end=\" \")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLR_S72lPhuW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}