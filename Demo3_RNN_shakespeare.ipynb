{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Demo3_RNN_shakespeare.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AhirtonLopes/TDCSP2019_Demos/blob/master/Demo3_RNN_shakespeare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuauS__EBg-o",
        "colab_type": "text"
      },
      "source": [
        "# Demo 3 - Gerador automatizado de textos no estilo de William Shakespeare\n",
        "\n",
        "Neste notebook iremos treinar uma Rede Neural Recorrente (RNN) tendo em vista indexação e posterior construção automatizada de textos teatrais no estilo de escrita do escrito inglês William Shakespeare (1564 - 1616) usando Pytorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbUyEL-EEGn7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importando bibliotecas utilizadas\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.cuda as cuda\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1476pwvEJjX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9c705f28-b755-420b-a044-68d48d8744da"
      },
      "source": [
        "# Baixando nossa base de dados a ser utilizada na demo\n",
        "\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdMOTSxTEL4i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Declarando nosso corpus textual a ser utilizado\n",
        "\n",
        "corpus = open(path_to_file, 'rb').read().decode(encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NO-F8VhJEU2q",
        "colab_type": "code",
        "outputId": "b02b7fcc-5bc3-4ea3-ef65-31336f953dbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "print(\"Numero de palavras no corpus textual: \",len(corpus))\n",
        "text=corpus.splitlines()\n",
        "print(\"Numero de linhas no corpus textual \",len(text))\n",
        "\n",
        "print(\"A primeira linha do corpus textual é => \",text[0])\n",
        "\n",
        "# Criando nossos conjuntos de treino, teste e validação\n",
        "\n",
        "for i in range(len(text)):\n",
        "  text[i]=text[i]+\" <end>\" ## adicionando <end> ao final de cada linha \n",
        "print(\"Agora a primeira linha do corpus textual é => \",text[0]) \n",
        "\n",
        "\n",
        "words=' '.join(text)\n",
        "words=words.split()\n",
        "print(\"Mostrando as palavras de posição 0 a 100 em nosso corpus : \",words[0:100])\n",
        "\n",
        "# Usamos essas palavras para criar vocabulário"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Numero de palavras no corpus textual:  1115394\n",
            "Numero de linhas no corpus textual  40000\n",
            "A primeira linha do corpus textual é =>  First Citizen:\n",
            "Agora a primeira linha do corpus textual é =>  First Citizen: <end>\n",
            "Mostrando as palavras de posição 0 a 100 em nosso corpus :  ['First', 'Citizen:', '<end>', 'Before', 'we', 'proceed', 'any', 'further,', 'hear', 'me', 'speak.', '<end>', '<end>', 'All:', '<end>', 'Speak,', 'speak.', '<end>', '<end>', 'First', 'Citizen:', '<end>', 'You', 'are', 'all', 'resolved', 'rather', 'to', 'die', 'than', 'to', 'famish?', '<end>', '<end>', 'All:', '<end>', 'Resolved.', 'resolved.', '<end>', '<end>', 'First', 'Citizen:', '<end>', 'First,', 'you', 'know', 'Caius', 'Marcius', 'is', 'chief', 'enemy', 'to', 'the', 'people.', '<end>', '<end>', 'All:', '<end>', 'We', \"know't,\", 'we', \"know't.\", '<end>', '<end>', 'First', 'Citizen:', '<end>', 'Let', 'us', 'kill', 'him,', 'and', \"we'll\", 'have', 'corn', 'at', 'our', 'own', 'price.', '<end>', \"Is't\", 'a', 'verdict?', '<end>', '<end>', 'All:', '<end>', 'No', 'more', 'talking', \"on't;\", 'let', 'it', 'be', 'done:', 'away,', 'away!', '<end>', '<end>', 'Second']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ga3Wtuq0xMx",
        "colab_type": "code",
        "outputId": "5c4dc2b4-10eb-4409-afc5-591cebbd0342",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# Criando vocabulário\n",
        "\n",
        "voc=[]\n",
        "\n",
        "def return_index(word):\n",
        "  return voc.index(word)\n",
        " \n",
        "for word in words :\n",
        "  if word not in voc:\n",
        "    voc.append(word)\n",
        "    \n",
        "print(\"Indice da palavra (The) no vocabulario: \",return_index('The') )\n",
        "print(\"A palavra para o indice de numero (203): \",voc[203] )\n",
        "\n",
        "print(\"Comprimento de nosso vocabulario: \",len(voc))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Indice da palavra (The) no vocabulario:  203\n",
            "A palavra para o indice de numero (203):  The\n",
            "Comprimento de nosso vocabulario:  25671\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJxYsGfx7OIT",
        "colab_type": "code",
        "outputId": "2eb7d400-4e95-4f15-aa7a-74717ebc605d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "# Convertendo palavras em índices e salvando em words_indexes\n",
        "\n",
        "words_indexes=[return_index(word) for word in words]\n",
        "print(\"Imprimindo alguns de nossos primeiros indices : \",words_indexes[0:100])\n",
        "\n",
        "# Utilizamos words_indexes como entrada para nossa rede\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Imprimindo alguns de nossos primeiros indices :  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 2, 11, 2, 12, 10, 2, 2, 0, 1, 2, 13, 14, 15, 16, 17, 18, 19, 20, 18, 21, 2, 2, 11, 2, 22, 23, 2, 2, 0, 1, 2, 24, 25, 26, 27, 28, 29, 30, 31, 18, 32, 33, 2, 2, 11, 2, 34, 35, 4, 36, 2, 2, 0, 1, 2, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 2, 49, 50, 51, 2, 2, 11, 2, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 2, 2, 62]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cjjZtu9-0o0",
        "colab_type": "code",
        "outputId": "fb179a06-2fdb-476f-bcf0-340ed647fb45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\n",
        "seq_lenght=35 # comprimento de nossa RNN (comprimento do input)\n",
        "batch_size=20\n",
        "# quantos batches(com tamanho: [batch_size, seq_lenght])  temos para nosso array words_indexes ?\n",
        "# resposta => len(words_indexes/(batch_size*seq_lenght))\n",
        "max_batches=int(len(words_indexes)/(batch_size*seq_lenght))\n",
        "print(\"O numero maximo de batches com essa configuracao e (batch_size,seq_lenght) :  \",max_batches)\n",
        "\n",
        "trainsetX=words_indexes[0:int(0.9*max_batches)*batch_size*seq_lenght]\n",
        "trainsetY=words_indexes[1:int(0.9*max_batches)*batch_size*seq_lenght+1]\n",
        "validsetX=words_indexes[int(0.9*max_batches)*batch_size*seq_lenght:int(1*max_batches)*batch_size*seq_lenght]\n",
        "validsetY=words_indexes[int(0.9*max_batches)*batch_size*seq_lenght + 1:int(1*max_batches)*batch_size*seq_lenght +1]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "O numero maximo de batches com essa configuracao e (batch_size,seq_lenght) :   346\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJqJCYHp0x_0",
        "colab_type": "code",
        "outputId": "77c0f68c-d829-408d-d3f7-053dd9819196",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "# Criando nossos batches\n",
        "\n",
        "def create_batch(data,flag):\n",
        "  if flag=='Train':\n",
        "     return (np.array(data).reshape(int(0.9*max_batches) ,batch_size ,seq_lenght)) # shape (número do batch, batch_size,seq_enght)\n",
        "  else :\n",
        "     return (np.array(data).reshape(int(1*max_batches)-int(0.9*max_batches) ,batch_size ,seq_lenght)) # shape (número do batch, batch_size,seq_enght)\n",
        "  \n",
        "trainX_batches =create_batch(trainsetX,'Train')\n",
        "trainY_batches =create_batch(trainsetY,'Train')\n",
        "\n",
        "validX_batches =create_batch(validsetX,'Valid')\n",
        "validY_batches =create_batch(validsetY,'Valid')\n",
        "\n",
        "\n",
        "print(\"trainX_batches_shape: \",trainX_batches.shape)\n",
        "print(\"trainY_batches_shape: \",trainY_batches.shape)\n",
        "\n",
        "print(\"validX_batches_shape: \",validX_batches.shape)\n",
        "print(\"validY_batches_shape: \",validY_batches.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trainX_batches_shape:  (311, 20, 35)\n",
            "trainY_batches_shape:  (311, 20, 35)\n",
            "validX_batches_shape:  (35, 20, 35)\n",
            "validY_batches_shape:  (35, 20, 35)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaCR9Gi0DEv0",
        "colab_type": "code",
        "outputId": "15edd1a0-c30d-4ba8-86d6-365d717accbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "print(trainX_batches[0,0,:]) # batchNumber=0 , amostra de número 0 no batch , em todos os comprimentos de sequência\n",
        "print(trainY_batches[0,0,:])\n",
        "print(len(words_indexes))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0  1  2  3  4  5  6  7  8  9 10  2  2 11  2 12 10  2  2  0  1  2 13 14\n",
            " 15 16 17 18 19 20 18 21  2  2 11]\n",
            "[ 1  2  3  4  5  6  7  8  9 10  2  2 11  2 12 10  2  2  0  1  2 13 14 15\n",
            " 16 17 18 19 20 18 21  2  2 11  2]\n",
            "242651\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBT0GZrS1IoH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Agregamos todas as informações em uma função\n",
        "\n",
        "def get_batches():# uso no treinamento\n",
        "  global  words_indexes\n",
        "  a=[]\n",
        "  b=[]\n",
        "  \n",
        "    # words_indexes shuffle\n",
        "  a=words_indexes[0:int(0.2*len(words_indexes))]\n",
        "  b=words_indexes[int(0.2*len(words_indexes)):]\n",
        "  words_indexes=b+a\n",
        "\n",
        "  trainsetX=words_indexes[0:int(0.9*max_batches)*batch_size*seq_lenght]\n",
        "  trainsetY=words_indexes[1:int(0.9*max_batches)*batch_size*seq_lenght+1]\n",
        "  validsetX=words_indexes[int(0.9*max_batches)*batch_size*seq_lenght:int(1*max_batches)*batch_size*seq_lenght]\n",
        "  validsetY=words_indexes[int(0.9*max_batches)*batch_size*seq_lenght + 1:int(1*max_batches)*batch_size*seq_lenght +1]\n",
        "\n",
        "  trainX_batches =create_batch(trainsetX,'Train')\n",
        "  trainY_batches =create_batch(trainsetY,'Train')\n",
        "  validX_batches =create_batch(validsetX,'Valid')\n",
        "  validY_batches =create_batch(validsetY,'Valid')\n",
        "  \n",
        "  return  trainX_batches,trainY_batches,validX_batches,validY_batches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gPNo1yy1I2i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Criando nossa rede RNN\n",
        "\n",
        "class net(torch.nn.Module):\n",
        "  def  __init__(self,vocab_size,embed_size,hidden_size,number_of_layers,num_dir):\n",
        "    super().__init__()\n",
        "    \n",
        "    # Se RNN for biderecional, num_directions será 2, senão 1\n",
        "  \n",
        "    self.num_directions=num_dir\n",
        "    self.number_of_layers=number_of_layers\n",
        "    self.hidden_size=hidden_size\n",
        "    \n",
        "    self.embd= torch.nn.Embedding(vocab_size, embed_size)\n",
        "    self.drop1=torch.nn.Dropout(0.3)\n",
        "    self.gru = torch.nn.GRU(embed_size, hidden_size, number_of_layers, dropout=0.4,bidirectional=False)\n",
        "    self.drop2=torch.nn.Dropout(0.1)\n",
        "    self.fc=torch.nn.Linear(hidden_size,vocab_size)\n",
        "    self.init_weights()\n",
        "\n",
        "  def forward(self,x,hidden):\n",
        "    \n",
        "    #x shape : torch.Size([seq_lenght, batch_size]) contem os índices de palavras de nosso vocabulário\n",
        "    #hidden shape :torch.Size([num_layers * num_directions, batch_size, hidden_size])\n",
        "    #média camada escondida h0\n",
        "    \n",
        "    emb=self.embd(x) \n",
        "    #emb shape : torch.Size([seq_lenght, batch_size, embed_size])\n",
        "    out=self.drop1(emb)\n",
        "    out , hidden =self.gru(emb,hidden) #  out = a camada escondidade mais alto nível para todos os steps |  hidden= contém valores para todas as camads escondidas do último timestep\n",
        "    #out shape :  torch.Size([seq_lenght, batch_size, hidden_size  * num_directions])\n",
        "    #hidden shape : torch.Size([num_layers * num_directions, batch_size, hidden_size])\n",
        "    \n",
        "    \n",
        "    out=self.drop2(out) # it doesnt  change the dimension\n",
        "   \n",
        "    output=out.view(out.size(0)* out.size(1),out.size(2))   \n",
        "    #out shape:  torch.Size([seq_lenght * batch_size, hidden_size  * num_directions])\n",
        "    \n",
        "    \n",
        "    output=self.fc(output)\n",
        "    #output shape:  torch.Size([seq_lenght * batch_size, vocab_size])\n",
        "    \n",
        "    \n",
        "    output=output.view(out.shape[0], out.shape[1],output.shape[1])   \n",
        "    #output shape:  torch.Size([seq_lenght , batch_size, vocab_size])\n",
        "\n",
        "    \n",
        "    # camada escondida é h0 no póximo processo de feed\n",
        "    return output , hidden\n",
        "  def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.embd.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.fill_(0)\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        \n",
        "  def init_hidden(self, batch_size): # inicialização com zeros\n",
        "        weight = next(self.parameters()).data\n",
        "        return torch.autograd.Variable(weight.new(self.number_of_layers, batch_size, self.hidden_size).zero_())\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stvpINrJ1JGU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size=len(voc)\n",
        "embed_size=250\n",
        "hidden_size=200\n",
        "number_of_layers=2\n",
        "num_direction=1\n",
        "learning_rate=0.001\n",
        "clip = 0.25"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AANGMmnTQjw0",
        "colab_type": "code",
        "outputId": "08c2ef27-884a-4251-dedf-eeaeca68e3c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "model=net(vocab_size,embed_size,hidden_size,number_of_layers,num_direction)\n",
        "print(model)\n",
        "if cuda.is_available():\n",
        "    model.cuda()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "net(\n",
            "  (embd): Embedding(25671, 250)\n",
            "  (drop1): Dropout(p=0.3)\n",
            "  (gru): GRU(250, 200, num_layers=2, dropout=0.4)\n",
            "  (drop2): Dropout(p=0.1)\n",
            "  (fc): Linear(in_features=200, out_features=25671, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTgsiNIyQls0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtQ1I_Kw5HHg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculating accuracy\n",
        "\n",
        "def cal_accuracy(pred_classes,real_classes):#  shape -> both : [number of test samples,1]  ou both : [number of test samples,]\n",
        "  bool_array=(pred_classes==real_classes) # exemplo: bool_array=[True, False, True, False, True]\n",
        "  True_pred_counts=np.count_nonzero(bool_array) # contando o número de verdadeiros em [True, False, True, False, True]\n",
        "  \n",
        "  return True_pred_counts/pred_classes.shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ap8AXWsIV1ad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#important notices:\n",
        "#a.shape=(2,3,4)\n",
        "#a.reshape(6,4) ->  (a000) (a001) (a002) (a003)      ,a021=a[0,2,1]\n",
        "#                   (a010) (a011) (a012) (a013) \n",
        "#                   (a020) (a021) (a022) (a023) \n",
        "#                   (a100) (a101) (a102) (a103) \n",
        "#                   (a110) (a111) (a112) (a113) \n",
        "#                   (a120) (a121) (a122) (a123) \n",
        "#\n",
        "#\n",
        "#a.shape=(3,2,4)\n",
        "#a.reshape(6,4) ->  (a000) (a001) (a002) (a003) \n",
        "#                   (a010) (a011) (a012) (a013) \n",
        "#                   (a100) (a101) (a102) (a103) \n",
        "#                   (a110) (a111) (a112) (a113) \n",
        "#                   (a200) (a201) (a202) (a203) \n",
        "#                   (a210) (a211) (a212) (a213) \n",
        "#\n",
        "#\n",
        "#a.shape=(3,2)\n",
        "#a.reshape(6,1) ->  (a00) \n",
        "#                   (a01)  \n",
        "#                   (a10)                   \n",
        "#                   (a11)  \n",
        "#                   (a20)  \n",
        "#                   (a21) \n",
        "#\n",
        "#\n",
        "#a.shape=(2,3)\n",
        "#a.reshape(6,1) ->  (a00) \n",
        "#                   (a01)  \n",
        "#                   (a02)                   \n",
        "#                   (a10)  \n",
        "#                   (a11)  \n",
        "#                   (a12) \n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a0rmFKiQmXG",
        "colab_type": "code",
        "outputId": "399f3e38-4c8d-4d7f-f53d-688cc5577e4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs=300\n",
        "mean_train_acc=[]\n",
        "mean_valid_acc=[]\n",
        "\n",
        "for i in range(num_epochs):\n",
        "  validacc=[]\n",
        "  trainacc=[]\n",
        "  hidden = model.init_hidden(batch_size)\n",
        "  total_loss=0\n",
        "  model.train()\n",
        "  trainX_batches,trainY_batches,_,_=get_batches()\n",
        "  for batchNumber in range(trainX_batches.shape[0]):\n",
        "    \n",
        "    X=torch.autograd.Variable(torch.LongTensor(trainX_batches[batchNumber].T)) \n",
        "    #train_batches[batchNumber].T  shape :nparray ([seq_lenght,batch_size]) \n",
        "    Y=torch.autograd.Variable(torch.LongTensor(trainY_batches[batchNumber].T.reshape(seq_lenght*batch_size)))  \n",
        "    #Y:Torch.szie ([seq_lenght*batch_size]) => seq0batch0  ,  seq0batch1 ,  seq0batch2  , ..... \n",
        "    if cuda.is_available():\n",
        "            X = X.cuda()\n",
        "            Y = Y.cuda()\n",
        "    \n",
        "    hidden = torch.autograd.Variable(hidden)# separando o estado escondido de uma iteração para outra\n",
        "    # essa em específico será usada para o plot de nosso gráfico computacional\n",
        "   \n",
        "    if cuda.is_available():\n",
        "             hidden = hidden.cuda()\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    outputs,hidden=model(X,hidden)# outputs shape:  torch.Size([seq_lenght , batch_size, vocab_size])\n",
        "    outputs=outputs.view(seq_lenght*batch_size, vocab_size)\n",
        "    #outputs shape:  torch.Size([seq_lenght * batch_size, vocab_size])=> seq0batch0,vocab  ,  seq0batch1,vocab ,  seq0batch2,vocab  , .....\n",
        "    \n",
        "    loss=criterion(outputs,Y)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm(model.parameters(), clip)\n",
        "    optimizer.step()\n",
        "\n",
        "    pred=outputs\n",
        "    pred_classes=torch.max(pred, 1)[1]# pred_classes.shape : torch.Size([seq_lenght * batch_size])\n",
        "    acc=cal_accuracy(np.array(pred_classes.cpu()),Y.cpu().numpy()) \n",
        "    trainacc.append(acc*100)\n",
        "\n",
        "  print(\"Acuracia de treinamento na epoca: \",i,\" e \",np.mean(trainacc))\n",
        "  mean_train_acc.append(np.mean(trainacc))\n",
        "  hidden = model.init_hidden(batch_size)\n",
        "  model.eval() \n",
        "  _,_,validX_batches,validY_batches=get_batches()\n",
        "\n",
        "  for batchNumber in range(validX_batches.shape[0]):\n",
        "    \n",
        "    X=torch.autograd.Variable(torch.LongTensor(validX_batches[batchNumber].T)) \n",
        "    #valid_batches[batchNumber].T  shape :nparray ([seq_lenght,batch_size]) \n",
        "    Y=torch.autograd.Variable(torch.LongTensor(validY_batches[batchNumber].T.reshape(seq_lenght*batch_size)))  \n",
        "    #Y:Torch.szie ([seq_lenght*batch_size]) \n",
        "    if cuda.is_available():\n",
        "            X = X.cuda()\n",
        "            Y = Y.cuda()\n",
        "  \n",
        "    if cuda.is_available():\n",
        "            hidden = hidden.cuda()\n",
        "      \n",
        "    outputs,hidden=model(X,hidden)\n",
        "    # outputs shape:  torch.Size([seq_lenght , batch_size, vocab_size])\n",
        "    outputs=outputs.view(seq_lenght*batch_size, vocab_size)\n",
        "    # outputs shape:  torch.Size([seq_lenght * batch_size, vocab_size])\n",
        "    pred=outputs \n",
        "    pred_classes=torch.max(pred.data, 1)[1]# torch.Size([seq_lenght * batch_size])\n",
        "    acc=cal_accuracy(np.array(pred_classes.cpu()),Y.cpu().numpy())\n",
        "    validacc.append(acc*100)\n",
        "\n",
        "  print(\"Acuracia de validacao na epoca: \",i,\" e \",np.mean(validacc))\n",
        "  mean_valid_acc.append(np.mean(validacc))\n",
        "\n",
        "  "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:35: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Acuracia de treinamento na epoca:  0  e  16.905374368396878\n",
            "Acuracia de validacao na epoca:  0  e  16.436734693877554\n",
            "Acuracia de treinamento na epoca:  1  e  19.45705098759761\n",
            "Acuracia de validacao na epoca:  1  e  19.22448979591837\n",
            "Acuracia de treinamento na epoca:  2  e  20.864033073036286\n",
            "Acuracia de validacao na epoca:  2  e  21.22448979591837\n",
            "Acuracia de treinamento na epoca:  3  e  22.345888837850254\n",
            "Acuracia de validacao na epoca:  3  e  22.281632653061227\n",
            "Acuracia de treinamento na epoca:  4  e  23.278824069820857\n",
            "Acuracia de validacao na epoca:  4  e  24.00816326530612\n",
            "Acuracia de treinamento na epoca:  5  e  23.883785025264125\n",
            "Acuracia de validacao na epoca:  5  e  23.004081632653065\n",
            "Acuracia de treinamento na epoca:  6  e  24.418925126320627\n",
            "Acuracia de validacao na epoca:  6  e  22.9265306122449\n",
            "Acuracia de treinamento na epoca:  7  e  24.927882406982082\n",
            "Acuracia de validacao na epoca:  7  e  24.448979591836732\n",
            "Acuracia de treinamento na epoca:  8  e  25.615066605420303\n",
            "Acuracia de validacao na epoca:  8  e  25.33061224489796\n",
            "Acuracia de treinamento na epoca:  9  e  26.372071658245293\n",
            "Acuracia de validacao na epoca:  9  e  25.795918367346935\n",
            "Acuracia de treinamento na epoca:  10  e  27.198438217730825\n",
            "Acuracia de validacao na epoca:  10  e  25.077551020408162\n",
            "Acuracia de treinamento na epoca:  11  e  27.861736334405144\n",
            "Acuracia de validacao na epoca:  11  e  24.502040816326534\n",
            "Acuracia de treinamento na epoca:  12  e  28.602204869085902\n",
            "Acuracia de validacao na epoca:  12  e  26.228571428571424\n",
            "Acuracia de treinamento na epoca:  13  e  29.832797427652732\n",
            "Acuracia de validacao na epoca:  13  e  27.428571428571427\n",
            "Acuracia de treinamento na epoca:  14  e  30.807073954983924\n",
            "Acuracia de validacao na epoca:  14  e  28.26938775510204\n",
            "Acuracia de treinamento na epoca:  15  e  32.08681672025723\n",
            "Acuracia de validacao na epoca:  15  e  26.59591836734694\n",
            "Acuracia de treinamento na epoca:  16  e  32.71428571428572\n",
            "Acuracia de validacao na epoca:  16  e  25.457142857142856\n",
            "Acuracia de treinamento na epoca:  17  e  33.71290767110703\n",
            "Acuracia de validacao na epoca:  17  e  28.665306122448982\n",
            "Acuracia de treinamento na epoca:  18  e  34.87965089572806\n",
            "Acuracia de validacao na epoca:  18  e  30.159183673469386\n",
            "Acuracia de treinamento na epoca:  19  e  36.106568672485075\n",
            "Acuracia de validacao na epoca:  19  e  31.526530612244894\n",
            "Acuracia de treinamento na epoca:  20  e  36.880569591180524\n",
            "Acuracia de validacao na epoca:  20  e  29.179591836734698\n",
            "Acuracia de treinamento na epoca:  21  e  37.83233807992651\n",
            "Acuracia de validacao na epoca:  21  e  29.122448979591834\n",
            "Acuracia de treinamento na epoca:  22  e  38.41616903996326\n",
            "Acuracia de validacao na epoca:  22  e  30.893877551020406\n",
            "Acuracia de treinamento na epoca:  23  e  39.597611391823605\n",
            "Acuracia de validacao na epoca:  23  e  31.51428571428571\n",
            "Acuracia de treinamento na epoca:  24  e  40.58842443729903\n",
            "Acuracia de validacao na epoca:  24  e  34.18367346938776\n",
            "Acuracia de treinamento na epoca:  25  e  41.3578318787322\n",
            "Acuracia de validacao na epoca:  25  e  31.2734693877551\n",
            "Acuracia de treinamento na epoca:  26  e  41.98254478640332\n",
            "Acuracia de validacao na epoca:  26  e  32.24489795918367\n",
            "Acuracia de treinamento na epoca:  27  e  42.33853927423059\n",
            "Acuracia de validacao na epoca:  27  e  33.99183673469387\n",
            "Acuracia de treinamento na epoca:  28  e  43.538814882866326\n",
            "Acuracia de validacao na epoca:  28  e  34.60408163265306\n",
            "Acuracia de treinamento na epoca:  29  e  44.23886081763895\n",
            "Acuracia de validacao na epoca:  29  e  39.1469387755102\n",
            "Acuracia de treinamento na epoca:  30  e  45.28709232889297\n",
            "Acuracia de validacao na epoca:  30  e  37.0938775510204\n",
            "Acuracia de treinamento na epoca:  31  e  45.49977032613689\n",
            "Acuracia de validacao na epoca:  31  e  35.73469387755102\n",
            "Acuracia de treinamento na epoca:  32  e  45.96738631143776\n",
            "Acuracia de validacao na epoca:  32  e  37.90612244897959\n",
            "Acuracia de treinamento na epoca:  33  e  46.70601745521359\n",
            "Acuracia de validacao na epoca:  33  e  36.1469387755102\n",
            "Acuracia de treinamento na epoca:  34  e  47.340376665135516\n",
            "Acuracia de validacao na epoca:  34  e  40.828571428571436\n",
            "Acuracia de treinamento na epoca:  35  e  48.35737253100597\n",
            "Acuracia de validacao na epoca:  35  e  37.83673469387755\n",
            "Acuracia de treinamento na epoca:  36  e  48.54294901240238\n",
            "Acuracia de validacao na epoca:  36  e  37.897959183673464\n",
            "Acuracia de treinamento na epoca:  37  e  48.92099219108866\n",
            "Acuracia de validacao na epoca:  37  e  42.37551020408163\n",
            "Acuracia de treinamento na epoca:  38  e  49.561322921451534\n",
            "Acuracia de validacao na epoca:  38  e  39.73061224489796\n",
            "Acuracia de treinamento na epoca:  39  e  50.28203950390446\n",
            "Acuracia de validacao na epoca:  39  e  43.07755102040817\n",
            "Acuracia de treinamento na epoca:  40  e  51.07946715663758\n",
            "Acuracia de validacao na epoca:  40  e  43.21632653061225\n",
            "Acuracia de treinamento na epoca:  41  e  51.22324299494718\n",
            "Acuracia de validacao na epoca:  41  e  42.791836734693874\n",
            "Acuracia de treinamento na epoca:  42  e  51.514928801102435\n",
            "Acuracia de validacao na epoca:  42  e  44.820408163265306\n",
            "Acuracia de treinamento na epoca:  43  e  52.21589343132751\n",
            "Acuracia de validacao na epoca:  43  e  42.01224489795918\n",
            "Acuracia de treinamento na epoca:  44  e  52.68305006890216\n",
            "Acuracia de validacao na epoca:  44  e  47.48979591836735\n",
            "Acuracia de treinamento na epoca:  45  e  53.57326596233349\n",
            "Acuracia de validacao na epoca:  45  e  47.293877551020415\n",
            "Acuracia de treinamento na epoca:  46  e  53.59623334864493\n",
            "Acuracia de validacao na epoca:  46  e  45.98367346938776\n",
            "Acuracia de treinamento na epoca:  47  e  53.97749196141479\n",
            "Acuracia de validacao na epoca:  47  e  49.506122448979596\n",
            "Acuracia de treinamento na epoca:  48  e  54.48461185117134\n",
            "Acuracia de validacao na epoca:  48  e  45.555102040816315\n",
            "Acuracia de treinamento na epoca:  49  e  54.97381717960496\n",
            "Acuracia de validacao na epoca:  49  e  51.28163265306123\n",
            "Acuracia de treinamento na epoca:  50  e  55.735875057418475\n",
            "Acuracia de validacao na epoca:  50  e  53.51020408163265\n",
            "Acuracia de treinamento na epoca:  51  e  55.57831878732201\n",
            "Acuracia de validacao na epoca:  51  e  46.90204081632653\n",
            "Acuracia de treinamento na epoca:  52  e  55.81717960496096\n",
            "Acuracia de validacao na epoca:  52  e  53.17551020408163\n",
            "Acuracia de treinamento na epoca:  53  e  56.565457050987604\n",
            "Acuracia de validacao na epoca:  53  e  47.963265306122445\n",
            "Acuracia de treinamento na epoca:  54  e  57.1612310519063\n",
            "Acuracia de validacao na epoca:  54  e  54.6\n",
            "Acuracia de treinamento na epoca:  55  e  57.70555810748737\n",
            "Acuracia de validacao na epoca:  55  e  54.42857142857142\n",
            "Acuracia de treinamento na epoca:  56  e  57.52595314653192\n",
            "Acuracia de validacao na epoca:  56  e  50.18367346938775\n",
            "Acuracia de treinamento na epoca:  57  e  57.77354157096922\n",
            "Acuracia de validacao na epoca:  57  e  55.526530612244905\n",
            "Acuracia de treinamento na epoca:  58  e  58.440514469453376\n",
            "Acuracia de validacao na epoca:  58  e  48.28571428571429\n",
            "Acuracia de treinamento na epoca:  59  e  58.964630225080384\n",
            "Acuracia de validacao na epoca:  59  e  53.86938775510204\n",
            "Acuracia de treinamento na epoca:  60  e  59.402388608176395\n",
            "Acuracia de validacao na epoca:  60  e  57.92244897959184\n",
            "Acuracia de treinamento na epoca:  61  e  59.5016077170418\n",
            "Acuracia de validacao na epoca:  61  e  53.946938775510205\n",
            "Acuracia de treinamento na epoca:  62  e  59.551217271474506\n",
            "Acuracia de validacao na epoca:  62  e  58.41224489795919\n",
            "Acuracia de treinamento na epoca:  63  e  60.28984841525034\n",
            "Acuracia de validacao na epoca:  63  e  52.90204081632652\n",
            "Acuracia de treinamento na epoca:  64  e  60.81258612769867\n",
            "Acuracia de validacao na epoca:  64  e  57.68979591836735\n",
            "Acuracia de treinamento na epoca:  65  e  61.18465778594396\n",
            "Acuracia de validacao na epoca:  65  e  59.73061224489797\n",
            "Acuracia de treinamento na epoca:  66  e  61.00137804317869\n",
            "Acuracia de validacao na epoca:  66  e  53.18775510204081\n",
            "Acuracia de treinamento na epoca:  67  e  61.073495636196604\n",
            "Acuracia de validacao na epoca:  67  e  59.59183673469388\n",
            "Acuracia de treinamento na epoca:  68  e  61.82958199356913\n",
            "Acuracia de validacao na epoca:  68  e  55.09387755102041\n",
            "Acuracia de treinamento na epoca:  69  e  62.29949471750114\n",
            "Acuracia de validacao na epoca:  69  e  62.91020408163265\n",
            "Acuracia de treinamento na epoca:  70  e  62.48139641708772\n",
            "Acuracia de validacao na epoca:  70  e  63.18775510204081\n",
            "Acuracia de treinamento na epoca:  71  e  62.64079007808911\n",
            "Acuracia de validacao na epoca:  71  e  59.261224489795914\n",
            "Acuracia de treinamento na epoca:  72  e  62.542030316949926\n",
            "Acuracia de validacao na epoca:  72  e  60.98775510204083\n",
            "Acuracia de treinamento na epoca:  73  e  63.147909967845656\n",
            "Acuracia de validacao na epoca:  73  e  56.68571428571429\n",
            "Acuracia de treinamento na epoca:  74  e  63.98851630684428\n",
            "Acuracia de validacao na epoca:  74  e  63.29795918367347\n",
            "Acuracia de treinamento na epoca:  75  e  63.925126320624706\n",
            "Acuracia de validacao na epoca:  75  e  65.86938775510204\n",
            "Acuracia de treinamento na epoca:  76  e  63.97565457050988\n",
            "Acuracia de validacao na epoca:  76  e  59.56734693877551\n",
            "Acuracia de treinamento na epoca:  77  e  63.907211759301795\n",
            "Acuracia de validacao na epoca:  77  e  64.78775510204082\n",
            "Acuracia de treinamento na epoca:  78  e  64.59715204409737\n",
            "Acuracia de validacao na epoca:  78  e  58.6\n",
            "Acuracia de treinamento na epoca:  79  e  65.10013780431787\n",
            "Acuracia de validacao na epoca:  79  e  66.0612244897959\n",
            "Acuracia de treinamento na epoca:  80  e  65.36196600826825\n",
            "Acuracia de validacao na epoca:  80  e  67.82040816326531\n",
            "Acuracia de treinamento na epoca:  81  e  65.26136885622417\n",
            "Acuracia de validacao na epoca:  81  e  59.23673469387755\n",
            "Acuracia de treinamento na epoca:  82  e  65.14882866329812\n",
            "Acuracia de validacao na epoca:  82  e  67.12244897959185\n",
            "Acuracia de treinamento na epoca:  83  e  65.69545245751033\n",
            "Acuracia de validacao na epoca:  83  e  59.012244897959185\n",
            "Acuracia de treinamento na epoca:  84  e  66.33027101515846\n",
            "Acuracia de validacao na epoca:  84  e  65.27755102040817\n",
            "Acuracia de treinamento na epoca:  85  e  66.42351860358292\n",
            "Acuracia de validacao na epoca:  85  e  65.72244897959183\n",
            "Acuracia de treinamento na epoca:  86  e  66.46072576940745\n",
            "Acuracia de validacao na epoca:  86  e  61.85306122448981\n",
            "Acuracia de treinamento na epoca:  87  e  66.35415709692236\n",
            "Acuracia de validacao na epoca:  87  e  67.6408163265306\n",
            "Acuracia de treinamento na epoca:  88  e  67.04363803399173\n",
            "Acuracia de validacao na epoca:  88  e  60.383673469387745\n",
            "Acuracia de treinamento na epoca:  89  e  67.63527790537437\n",
            "Acuracia de validacao na epoca:  89  e  67.11428571428571\n",
            "Acuracia de treinamento na epoca:  90  e  67.58980248047773\n",
            "Acuracia de validacao na epoca:  90  e  69.75510204081633\n",
            "Acuracia de treinamento na epoca:  91  e  67.61782269177769\n",
            "Acuracia de validacao na epoca:  91  e  64.26122448979592\n",
            "Acuracia de treinamento na epoca:  92  e  67.59898943500231\n",
            "Acuracia de validacao na epoca:  92  e  68.78775510204082\n",
            "Acuracia de treinamento na epoca:  93  e  68.07762976573265\n",
            "Acuracia de validacao na epoca:  93  e  62.6\n",
            "Acuracia de treinamento na epoca:  94  e  68.32843362425355\n",
            "Acuracia de validacao na epoca:  94  e  65.87755102040816\n",
            "Acuracia de treinamento na epoca:  95  e  68.65273311897106\n",
            "Acuracia de validacao na epoca:  95  e  71.22448979591834\n",
            "Acuracia de treinamento na epoca:  96  e  68.60174552135966\n",
            "Acuracia de validacao na epoca:  96  e  64.88979591836734\n",
            "Acuracia de treinamento na epoca:  97  e  68.55351401010566\n",
            "Acuracia de validacao na epoca:  97  e  67.79591836734691\n",
            "Acuracia de treinamento na epoca:  98  e  68.96554892053284\n",
            "Acuracia de validacao na epoca:  98  e  63.938775510204074\n",
            "Acuracia de treinamento na epoca:  99  e  69.53100597152043\n",
            "Acuracia de validacao na epoca:  99  e  69.71836734693878\n",
            "Acuracia de treinamento na epoca:  100  e  69.70234267340378\n",
            "Acuracia de validacao na epoca:  100  e  72.03673469387756\n",
            "Acuracia de treinamento na epoca:  101  e  69.53100597152043\n",
            "Acuracia de validacao na epoca:  101  e  66.66938775510205\n",
            "Acuracia de treinamento na epoca:  102  e  69.46118511713367\n",
            "Acuracia de validacao na epoca:  102  e  71.73877551020409\n",
            "Acuracia de treinamento na epoca:  103  e  69.99540652273771\n",
            "Acuracia de validacao na epoca:  103  e  66.48979591836734\n",
            "Acuracia de treinamento na epoca:  104  e  70.65365181442353\n",
            "Acuracia de validacao na epoca:  104  e  70.47755102040817\n",
            "Acuracia de treinamento na epoca:  105  e  70.60725769407442\n",
            "Acuracia de validacao na epoca:  105  e  71.78775510204082\n",
            "Acuracia de treinamento na epoca:  106  e  70.65778594395957\n",
            "Acuracia de validacao na epoca:  106  e  67.91428571428571\n",
            "Acuracia de treinamento na epoca:  107  e  70.2682590721176\n",
            "Acuracia de validacao na epoca:  107  e  71.68163265306123\n",
            "Acuracia de treinamento na epoca:  108  e  70.87367937528708\n",
            "Acuracia de validacao na epoca:  108  e  66.86530612244898\n",
            "Acuracia de treinamento na epoca:  109  e  71.31097841065689\n",
            "Acuracia de validacao na epoca:  109  e  71.6734693877551\n",
            "Acuracia de treinamento na epoca:  110  e  71.63849333945797\n",
            "Acuracia de validacao na epoca:  110  e  73.67755102040815\n",
            "Acuracia de treinamento na epoca:  111  e  71.48874598070739\n",
            "Acuracia de validacao na epoca:  111  e  69.72653061224489\n",
            "Acuracia de treinamento na epoca:  112  e  71.35094166283876\n",
            "Acuracia de validacao na epoca:  112  e  76.41632653061222\n",
            "Acuracia de treinamento na epoca:  113  e  71.5957740009187\n",
            "Acuracia de validacao na epoca:  113  e  69.04081632653062\n",
            "Acuracia de treinamento na epoca:  114  e  72.11024345429492\n",
            "Acuracia de validacao na epoca:  114  e  74.37142857142857\n",
            "Acuracia de treinamento na epoca:  115  e  72.42994947175012\n",
            "Acuracia de validacao na epoca:  115  e  75.46938775510203\n",
            "Acuracia de treinamento na epoca:  116  e  72.23059255856683\n",
            "Acuracia de validacao na epoca:  116  e  72.68163265306121\n",
            "Acuracia de treinamento na epoca:  117  e  71.95314653192467\n",
            "Acuracia de validacao na epoca:  117  e  76.60816326530612\n",
            "Acuracia de treinamento na epoca:  118  e  72.49747358750574\n",
            "Acuracia de validacao na epoca:  118  e  67.26938775510203\n",
            "Acuracia de treinamento na epoca:  119  e  72.93339457969682\n",
            "Acuracia de validacao na epoca:  119  e  74.35102040816327\n",
            "Acuracia de treinamento na epoca:  120  e  72.93339457969682\n",
            "Acuracia de validacao na epoca:  120  e  77.15102040816328\n",
            "Acuracia de treinamento na epoca:  121  e  72.97197978870004\n",
            "Acuracia de validacao na epoca:  121  e  71.46122448979592\n",
            "Acuracia de treinamento na epoca:  122  e  72.78318787322003\n",
            "Acuracia de validacao na epoca:  122  e  76.19591836734693\n",
            "Acuracia de treinamento na epoca:  123  e  73.13826366559485\n",
            "Acuracia de validacao na epoca:  123  e  69.20816326530614\n",
            "Acuracia de treinamento na epoca:  124  e  73.56959118052364\n",
            "Acuracia de validacao na epoca:  124  e  77.13061224489797\n",
            "Acuracia de treinamento na epoca:  125  e  73.59669269637115\n",
            "Acuracia de validacao na epoca:  125  e  77.05714285714286\n",
            "Acuracia de treinamento na epoca:  126  e  73.54524575103353\n",
            "Acuracia de validacao na epoca:  126  e  73.97551020408164\n",
            "Acuracia de treinamento na epoca:  127  e  73.51538814882866\n",
            "Acuracia de validacao na epoca:  127  e  76.72653061224489\n",
            "Acuracia de treinamento na epoca:  128  e  73.92466697289848\n",
            "Acuracia de validacao na epoca:  128  e  70.31836734693877\n",
            "Acuracia de treinamento na epoca:  129  e  74.3977951309141\n",
            "Acuracia de validacao na epoca:  129  e  77.86938775510203\n",
            "Acuracia de treinamento na epoca:  130  e  74.22140560404226\n",
            "Acuracia de validacao na epoca:  130  e  75.84081632653061\n",
            "Acuracia de treinamento na epoca:  131  e  74.12356453835554\n",
            "Acuracia de validacao na epoca:  131  e  73.48163265306123\n",
            "Acuracia de treinamento na epoca:  132  e  74.19797887000459\n",
            "Acuracia de validacao na epoca:  132  e  76.39183673469388\n",
            "Acuracia de treinamento na epoca:  133  e  74.437758383096\n",
            "Acuracia de validacao na epoca:  133  e  69.4\n",
            "Acuracia de treinamento na epoca:  134  e  74.99540652273772\n",
            "Acuracia de validacao na epoca:  134  e  77.93469387755103\n",
            "Acuracia de treinamento na epoca:  135  e  74.96554892053284\n",
            "Acuracia de validacao na epoca:  135  e  79.72244897959185\n",
            "Acuracia de treinamento na epoca:  136  e  74.98943500229674\n",
            "Acuracia de validacao na epoca:  136  e  76.41632653061222\n",
            "Acuracia de treinamento na epoca:  137  e  74.68626550298576\n",
            "Acuracia de validacao na epoca:  137  e  78.60408163265306\n",
            "Acuracia de treinamento na epoca:  138  e  75.12034910427194\n",
            "Acuracia de validacao na epoca:  138  e  72.19183673469388\n",
            "Acuracia de treinamento na epoca:  139  e  75.38906752411576\n",
            "Acuracia de validacao na epoca:  139  e  77.04897959183673\n",
            "Acuracia de treinamento na epoca:  140  e  75.5062011943041\n",
            "Acuracia de validacao na epoca:  140  e  79.60408163265305\n",
            "Acuracia de treinamento na epoca:  141  e  75.54111162149748\n",
            "Acuracia de validacao na epoca:  141  e  78.19183673469388\n",
            "Acuracia de treinamento na epoca:  142  e  75.15480018373908\n",
            "Acuracia de validacao na epoca:  142  e  77.24897959183674\n",
            "Acuracia de treinamento na epoca:  143  e  75.58704639412035\n",
            "Acuracia de validacao na epoca:  143  e  72.85714285714288\n",
            "Acuracia de treinamento na epoca:  144  e  76.11667432246212\n",
            "Acuracia de validacao na epoca:  144  e  78.82448979591838\n",
            "Acuracia de treinamento na epoca:  145  e  76.04731281580156\n",
            "Acuracia de validacao na epoca:  145  e  80.63265306122449\n",
            "Acuracia de treinamento na epoca:  146  e  75.90445567294442\n",
            "Acuracia de validacao na epoca:  146  e  78.04897959183673\n",
            "Acuracia de treinamento na epoca:  147  e  75.7698667891594\n",
            "Acuracia de validacao na epoca:  147  e  80.75102040816326\n",
            "Acuracia de treinamento na epoca:  148  e  76.1212677997244\n",
            "Acuracia de validacao na epoca:  148  e  74.78775510204082\n",
            "Acuracia de treinamento na epoca:  149  e  76.78089113458888\n",
            "Acuracia de validacao na epoca:  149  e  78.24489795918369\n",
            "Acuracia de treinamento na epoca:  150  e  76.56132292145153\n",
            "Acuracia de validacao na epoca:  150  e  81.16734693877551\n",
            "Acuracia de treinamento na epoca:  151  e  76.52916858061552\n",
            "Acuracia de validacao na epoca:  151  e  77.33469387755102\n",
            "Acuracia de treinamento na epoca:  152  e  76.17501148369315\n",
            "Acuracia de validacao na epoca:  152  e  80.36734693877551\n",
            "Acuracia de treinamento na epoca:  153  e  76.37436839687643\n",
            "Acuracia de validacao na epoca:  153  e  75.02448979591837\n",
            "Acuracia de treinamento na epoca:  154  e  77.19614147909968\n",
            "Acuracia de validacao na epoca:  154  e  77.83265306122449\n",
            "Acuracia de treinamento na epoca:  155  e  76.83601286173634\n",
            "Acuracia de validacao na epoca:  155  e  82.06530612244896\n",
            "Acuracia de treinamento na epoca:  156  e  76.94855305466238\n",
            "Acuracia de validacao na epoca:  156  e  76.03673469387756\n",
            "Acuracia de treinamento na epoca:  157  e  76.55764813964171\n",
            "Acuracia de validacao na epoca:  157  e  80.36734693877551\n",
            "Acuracia de treinamento na epoca:  158  e  76.90813045475426\n",
            "Acuracia de validacao na epoca:  158  e  74.44489795918368\n",
            "Acuracia de treinamento na epoca:  159  e  77.55167661920073\n",
            "Acuracia de validacao na epoca:  159  e  79.26530612244898\n",
            "Acuracia de treinamento na epoca:  160  e  77.39090491502067\n",
            "Acuracia de validacao na epoca:  160  e  79.97142857142858\n",
            "Acuracia de treinamento na epoca:  161  e  77.29536058796509\n",
            "Acuracia de validacao na epoca:  161  e  77.63673469387754\n",
            "Acuracia de treinamento na epoca:  162  e  77.2714745062012\n",
            "Acuracia de validacao na epoca:  162  e  80.78367346938776\n",
            "Acuracia de treinamento na epoca:  163  e  77.39549839228296\n",
            "Acuracia de validacao na epoca:  163  e  74.76326530612245\n",
            "Acuracia de treinamento na epoca:  164  e  77.83325677537896\n",
            "Acuracia de validacao na epoca:  164  e  80.2\n",
            "Acuracia de treinamento na epoca:  165  e  77.83647220946257\n",
            "Acuracia de validacao na epoca:  165  e  80.7265306122449\n",
            "Acuracia de treinamento na epoca:  166  e  77.8033991731741\n",
            "Acuracia de validacao na epoca:  166  e  81.04489795918367\n",
            "Acuracia de treinamento na epoca:  167  e  77.53054662379421\n",
            "Acuracia de validacao na epoca:  167  e  82.58775510204083\n",
            "Acuracia de treinamento na epoca:  168  e  77.7795130914102\n",
            "Acuracia de validacao na epoca:  168  e  75.72653061224489\n",
            "Acuracia de treinamento na epoca:  169  e  78.34864492420763\n",
            "Acuracia de validacao na epoca:  169  e  80.27755102040818\n",
            "Acuracia de treinamento na epoca:  170  e  78.2907671107028\n",
            "Acuracia de validacao na epoca:  170  e  82.28571428571429\n",
            "Acuracia de treinamento na epoca:  171  e  78.140560404226\n",
            "Acuracia de validacao na epoca:  171  e  78.81224489795919\n",
            "Acuracia de treinamento na epoca:  172  e  77.83601286173634\n",
            "Acuracia de validacao na epoca:  172  e  80.6204081632653\n",
            "Acuracia de treinamento na epoca:  173  e  78.07487367937529\n",
            "Acuracia de validacao na epoca:  173  e  77.66122448979593\n",
            "Acuracia de treinamento na epoca:  174  e  78.6821313734497\n",
            "Acuracia de validacao na epoca:  174  e  82.09795918367347\n",
            "Acuracia de treinamento na epoca:  175  e  78.64676159853008\n",
            "Acuracia de validacao na epoca:  175  e  82.84897959183674\n",
            "Acuracia de treinamento na epoca:  176  e  78.50390445567295\n",
            "Acuracia de validacao na epoca:  176  e  77.8734693877551\n",
            "Acuracia de treinamento na epoca:  177  e  78.33394579696831\n",
            "Acuracia de validacao na epoca:  177  e  83.76734693877549\n",
            "Acuracia de treinamento na epoca:  178  e  78.63435920992193\n",
            "Acuracia de validacao na epoca:  178  e  77.34693877551022\n",
            "Acuracia de treinamento na epoca:  179  e  78.90032154340837\n",
            "Acuracia de validacao na epoca:  179  e  82.28163265306124\n",
            "Acuracia de treinamento na epoca:  180  e  78.89894350022968\n",
            "Acuracia de validacao na epoca:  180  e  81.97551020408164\n",
            "Acuracia de treinamento na epoca:  181  e  78.7781350482315\n",
            "Acuracia de validacao na epoca:  181  e  78.61224489795917\n",
            "Acuracia de treinamento na epoca:  182  e  78.7602204869086\n",
            "Acuracia de validacao na epoca:  182  e  85.02448979591836\n",
            "Acuracia de treinamento na epoca:  183  e  78.86173633440515\n",
            "Acuracia de validacao na epoca:  183  e  77.95102040816326\n",
            "Acuracia de treinamento na epoca:  184  e  79.28020211299953\n",
            "Acuracia de validacao na epoca:  184  e  81.72244897959183\n",
            "Acuracia de treinamento na epoca:  185  e  79.40009186954524\n",
            "Acuracia de validacao na epoca:  185  e  84.33061224489796\n",
            "Acuracia de treinamento na epoca:  186  e  79.24161690399632\n",
            "Acuracia de validacao na epoca:  186  e  79.58775510204083\n",
            "Acuracia de treinamento na epoca:  187  e  78.76940744143316\n",
            "Acuracia de validacao na epoca:  187  e  84.76326530612243\n",
            "Acuracia de treinamento na epoca:  188  e  79.30684428112082\n",
            "Acuracia de validacao na epoca:  188  e  78.02448979591838\n",
            "Acuracia de treinamento na epoca:  189  e  79.62149747358751\n",
            "Acuracia de validacao na epoca:  189  e  82.11020408163266\n",
            "Acuracia de treinamento na epoca:  190  e  79.55167661920073\n",
            "Acuracia de validacao na epoca:  190  e  82.16734693877551\n",
            "Acuracia de treinamento na epoca:  191  e  79.61874138723013\n",
            "Acuracia de validacao na epoca:  191  e  77.50204081632654\n",
            "Acuracia de treinamento na epoca:  192  e  79.1148369315572\n",
            "Acuracia de validacao na epoca:  192  e  82.31428571428572\n",
            "Acuracia de treinamento na epoca:  193  e  79.64262746899404\n",
            "Acuracia de validacao na epoca:  193  e  77.71020408163265\n",
            "Acuracia de treinamento na epoca:  194  e  79.84565916398714\n",
            "Acuracia de validacao na epoca:  194  e  82.65306122448979\n",
            "Acuracia de treinamento na epoca:  195  e  79.77124483233808\n",
            "Acuracia de validacao na epoca:  195  e  85.8612244897959\n",
            "Acuracia de treinamento na epoca:  196  e  79.64997703261369\n",
            "Acuracia de validacao na epoca:  196  e  78.91020408163267\n",
            "Acuracia de treinamento na epoca:  197  e  79.50666054203032\n",
            "Acuracia de validacao na epoca:  197  e  84.53469387755102\n",
            "Acuracia de treinamento na epoca:  198  e  79.87551676619202\n",
            "Acuracia de validacao na epoca:  198  e  78.3265306122449\n",
            "Acuracia de treinamento na epoca:  199  e  80.17317409278824\n",
            "Acuracia de validacao na epoca:  199  e  81.88979591836734\n",
            "Acuracia de treinamento na epoca:  200  e  80.31786862655031\n",
            "Acuracia de validacao na epoca:  200  e  84.81632653061224\n",
            "Acuracia de treinamento na epoca:  201  e  80.10886541111621\n",
            "Acuracia de validacao na epoca:  201  e  77.46938775510205\n",
            "Acuracia de treinamento na epoca:  202  e  79.95498392282958\n",
            "Acuracia de validacao na epoca:  202  e  82.90612244897959\n",
            "Acuracia de treinamento na epoca:  203  e  80.10151584749656\n",
            "Acuracia de validacao na epoca:  203  e  78.04897959183674\n",
            "Acuracia de treinamento na epoca:  204  e  80.33118971061093\n",
            "Acuracia de validacao na epoca:  204  e  82.09795918367347\n",
            "Acuracia de treinamento na epoca:  205  e  80.33027101515849\n",
            "Acuracia de validacao na epoca:  205  e  83.68571428571428\n",
            "Acuracia de treinamento na epoca:  206  e  80.30546623794213\n",
            "Acuracia de validacao na epoca:  206  e  79.62857142857142\n",
            "Acuracia de treinamento na epoca:  207  e  80.33486449242076\n",
            "Acuracia de validacao na epoca:  207  e  83.1265306122449\n",
            "Acuracia de treinamento na epoca:  208  e  80.3945796968305\n",
            "Acuracia de validacao na epoca:  208  e  78.66938775510205\n",
            "Acuracia de treinamento na epoca:  209  e  80.71107028020211\n",
            "Acuracia de validacao na epoca:  209  e  80.75510204081631\n",
            "Acuracia de treinamento na epoca:  210  e  80.66054203031695\n",
            "Acuracia de validacao na epoca:  210  e  86.34285714285714\n",
            "Acuracia de treinamento na epoca:  211  e  80.59807073954984\n",
            "Acuracia de validacao na epoca:  211  e  81.50204081632654\n",
            "Acuracia de treinamento na epoca:  212  e  80.51722553973359\n",
            "Acuracia de validacao na epoca:  212  e  85.33877551020407\n",
            "Acuracia de treinamento na epoca:  213  e  80.72622875516767\n",
            "Acuracia de validacao na epoca:  213  e  78.88979591836737\n",
            "Acuracia de treinamento na epoca:  214  e  81.04915020670647\n",
            "Acuracia de validacao na epoca:  214  e  81.97959183673468\n",
            "Acuracia de treinamento na epoca:  215  e  80.8897565457051\n",
            "Acuracia de validacao na epoca:  215  e  86.71836734693878\n",
            "Acuracia de treinamento na epoca:  216  e  80.6936150666054\n",
            "Acuracia de validacao na epoca:  216  e  79.88571428571429\n",
            "Acuracia de treinamento na epoca:  217  e  80.63757464400553\n",
            "Acuracia de validacao na epoca:  217  e  84.62040816326531\n",
            "Acuracia de treinamento na epoca:  218  e  80.87827285254937\n",
            "Acuracia de validacao na epoca:  218  e  79.62448979591836\n",
            "Acuracia de treinamento na epoca:  219  e  81.15388148828664\n",
            "Acuracia de validacao na epoca:  219  e  83.39591836734694\n",
            "Acuracia de treinamento na epoca:  220  e  81.21727147450619\n",
            "Acuracia de validacao na epoca:  220  e  87.62448979591838\n",
            "Acuracia de treinamento na epoca:  221  e  81.04685346807534\n",
            "Acuracia de validacao na epoca:  221  e  83.0612244897959\n",
            "Acuracia de treinamento na epoca:  222  e  80.664676159853\n",
            "Acuracia de validacao na epoca:  222  e  84.4\n",
            "Acuracia de treinamento na epoca:  223  e  81.09921910886541\n",
            "Acuracia de validacao na epoca:  223  e  78.4857142857143\n",
            "Acuracia de treinamento na epoca:  224  e  81.36012861736334\n",
            "Acuracia de validacao na epoca:  224  e  81.96734693877552\n",
            "Acuracia de treinamento na epoca:  225  e  81.43224621038127\n",
            "Acuracia de validacao na epoca:  225  e  87.81224489795919\n",
            "Acuracia de treinamento na epoca:  226  e  81.16674322462103\n",
            "Acuracia de validacao na epoca:  226  e  79.62040816326532\n",
            "Acuracia de treinamento na epoca:  227  e  80.8801102434543\n",
            "Acuracia de validacao na epoca:  227  e  85.33061224489795\n",
            "Acuracia de treinamento na epoca:  228  e  81.37023426734038\n",
            "Acuracia de validacao na epoca:  228  e  78.26938775510204\n",
            "Acuracia de treinamento na epoca:  229  e  81.56499770326137\n",
            "Acuracia de validacao na epoca:  229  e  81.41224489795917\n",
            "Acuracia de treinamento na epoca:  230  e  81.57602204869086\n",
            "Acuracia de validacao na epoca:  230  e  86.51428571428572\n",
            "Acuracia de treinamento na epoca:  231  e  81.43959577400092\n",
            "Acuracia de validacao na epoca:  231  e  79.62040816326531\n",
            "Acuracia de treinamento na epoca:  232  e  81.02893890675242\n",
            "Acuracia de validacao na epoca:  232  e  85.40408163265306\n",
            "Acuracia de treinamento na epoca:  233  e  81.54203031694993\n",
            "Acuracia de validacao na epoca:  233  e  79.78367346938775\n",
            "Acuracia de treinamento na epoca:  234  e  81.69591180523656\n",
            "Acuracia de validacao na epoca:  234  e  82.6530612244898\n",
            "Acuracia de treinamento na epoca:  235  e  81.7666513550758\n",
            "Acuracia de validacao na epoca:  235  e  86.24489795918369\n",
            "Acuracia de treinamento na epoca:  236  e  81.63803399173175\n",
            "Acuracia de validacao na epoca:  236  e  80.51836734693877\n",
            "Acuracia de treinamento na epoca:  237  e  81.51033532384014\n",
            "Acuracia de validacao na epoca:  237  e  85.33469387755102\n",
            "Acuracia de treinamento na epoca:  238  e  81.72622875516767\n",
            "Acuracia de validacao na epoca:  238  e  79.37959183673469\n",
            "Acuracia de treinamento na epoca:  239  e  81.90629306384935\n",
            "Acuracia de validacao na epoca:  239  e  84.55918367346939\n",
            "Acuracia de treinamento na epoca:  240  e  82.02572347266882\n",
            "Acuracia de validacao na epoca:  240  e  85.82857142857142\n",
            "Acuracia de treinamento na epoca:  241  e  81.773082223243\n",
            "Acuracia de validacao na epoca:  241  e  77.91428571428573\n",
            "Acuracia de treinamento na epoca:  242  e  81.67202572347267\n",
            "Acuracia de validacao na epoca:  242  e  84.60000000000001\n",
            "Acuracia de treinamento na epoca:  243  e  81.98530087276069\n",
            "Acuracia de validacao na epoca:  243  e  82.16326530612245\n",
            "Acuracia de treinamento na epoca:  244  e  82.22737712448323\n",
            "Acuracia de validacao na epoca:  244  e  82.86938775510203\n",
            "Acuracia de treinamento na epoca:  245  e  82.07717041800643\n",
            "Acuracia de validacao na epoca:  245  e  85.55918367346938\n",
            "Acuracia de treinamento na epoca:  246  e  82.13367018833256\n",
            "Acuracia de validacao na epoca:  246  e  80.95510204081633\n",
            "Acuracia de treinamento na epoca:  247  e  81.70555810748738\n",
            "Acuracia de validacao na epoca:  247  e  84.86938775510203\n",
            "Acuracia de treinamento na epoca:  248  e  82.00229673863116\n",
            "Acuracia de validacao na epoca:  248  e  80.0612244897959\n",
            "Acuracia de treinamento na epoca:  249  e  82.29949471750115\n",
            "Acuracia de validacao na epoca:  249  e  82.37142857142858\n",
            "Acuracia de treinamento na epoca:  250  e  82.46531924666974\n",
            "Acuracia de validacao na epoca:  250  e  87.86938775510204\n",
            "Acuracia de treinamento na epoca:  251  e  82.17868626550299\n",
            "Acuracia de validacao na epoca:  251  e  80.11020408163265\n",
            "Acuracia de treinamento na epoca:  252  e  81.83876894809372\n",
            "Acuracia de validacao na epoca:  252  e  85.5265306122449\n",
            "Acuracia de treinamento na epoca:  253  e  82.13458888378503\n",
            "Acuracia de validacao na epoca:  253  e  80.21224489795918\n",
            "Acuracia de treinamento na epoca:  254  e  82.4441892512632\n",
            "Acuracia de validacao na epoca:  254  e  85.22857142857144\n",
            "Acuracia de treinamento na epoca:  255  e  82.36610013780432\n",
            "Acuracia de validacao na epoca:  255  e  87.35102040816325\n",
            "Acuracia de treinamento na epoca:  256  e  82.07165824529169\n",
            "Acuracia de validacao na epoca:  256  e  81.6326530612245\n",
            "Acuracia de treinamento na epoca:  257  e  82.03904455672945\n",
            "Acuracia de validacao na epoca:  257  e  87.80408163265307\n",
            "Acuracia de treinamento na epoca:  258  e  82.2347266881029\n",
            "Acuracia de validacao na epoca:  258  e  81.04081632653062\n",
            "Acuracia de treinamento na epoca:  259  e  82.70555810748738\n",
            "Acuracia de validacao na epoca:  259  e  82.50204081632653\n",
            "Acuracia de treinamento na epoca:  260  e  82.6421681212678\n",
            "Acuracia de validacao na epoca:  260  e  88.62857142857142\n",
            "Acuracia de treinamento na epoca:  261  e  82.52135966926963\n",
            "Acuracia de validacao na epoca:  261  e  80.4938775510204\n",
            "Acuracia de treinamento na epoca:  262  e  82.38723013321085\n",
            "Acuracia de validacao na epoca:  262  e  85.8204081632653\n",
            "Acuracia de treinamento na epoca:  263  e  82.48782728525494\n",
            "Acuracia de validacao na epoca:  263  e  81.68571428571428\n",
            "Acuracia de treinamento na epoca:  264  e  82.44143316490585\n",
            "Acuracia de validacao na epoca:  264  e  82.50204081632653\n",
            "Acuracia de treinamento na epoca:  265  e  82.73954983922829\n",
            "Acuracia de validacao na epoca:  265  e  87.48163265306123\n",
            "Acuracia de treinamento na epoca:  266  e  82.66008268259073\n",
            "Acuracia de validacao na epoca:  266  e  81.71020408163264\n",
            "Acuracia de treinamento na epoca:  267  e  82.32935231970602\n",
            "Acuracia de validacao na epoca:  267  e  85.86530612244898\n",
            "Acuracia de treinamento na epoca:  268  e  82.73082223242996\n",
            "Acuracia de validacao na epoca:  268  e  81.29795918367347\n",
            "Acuracia de treinamento na epoca:  269  e  82.64262746899404\n",
            "Acuracia de validacao na epoca:  269  e  83.5061224489796\n",
            "Acuracia de treinamento na epoca:  270  e  82.96417087735416\n",
            "Acuracia de validacao na epoca:  270  e  86.62040816326531\n",
            "Acuracia de treinamento na epoca:  271  e  82.89664676159853\n",
            "Acuracia de validacao na epoca:  271  e  82.97551020408164\n",
            "Acuracia de treinamento na epoca:  272  e  82.58153422140559\n",
            "Acuracia de validacao na epoca:  272  e  86.85306122448979\n",
            "Acuracia de treinamento na epoca:  273  e  82.84703720716583\n",
            "Acuracia de validacao na epoca:  273  e  80.87755102040818\n",
            "Acuracia de treinamento na epoca:  274  e  83.12586127698667\n",
            "Acuracia de validacao na epoca:  274  e  81.14693877551022\n",
            "Acuracia de treinamento na epoca:  275  e  83.05466237942122\n",
            "Acuracia de validacao na epoca:  275  e  89.48571428571428\n",
            "Acuracia de treinamento na epoca:  276  e  83.01010564997702\n",
            "Acuracia de validacao na epoca:  276  e  83.10204081632652\n",
            "Acuracia de treinamento na epoca:  277  e  82.54570509875975\n",
            "Acuracia de validacao na epoca:  277  e  86.57142857142857\n",
            "Acuracia de treinamento na epoca:  278  e  82.85209003215434\n",
            "Acuracia de validacao na epoca:  278  e  84.06938775510204\n",
            "Acuracia de treinamento na epoca:  279  e  83.22140560404226\n",
            "Acuracia de validacao na epoca:  279  e  84.91020408163264\n",
            "Acuracia de treinamento na epoca:  280  e  83.10840606338999\n",
            "Acuracia de validacao na epoca:  280  e  90.07755102040817\n",
            "Acuracia de treinamento na epoca:  281  e  83.16674322462104\n",
            "Acuracia de validacao na epoca:  281  e  83.73469387755102\n",
            "Acuracia de treinamento na epoca:  282  e  82.93017914561322\n",
            "Acuracia de validacao na epoca:  282  e  86.10204081632654\n",
            "Acuracia de treinamento na epoca:  283  e  83.016077170418\n",
            "Acuracia de validacao na epoca:  283  e  82.52244897959184\n",
            "Acuracia de treinamento na epoca:  284  e  83.26688102893891\n",
            "Acuracia de validacao na epoca:  284  e  82.05714285714286\n",
            "Acuracia de treinamento na epoca:  285  e  83.18006430868168\n",
            "Acuracia de validacao na epoca:  285  e  88.05714285714285\n",
            "Acuracia de treinamento na epoca:  286  e  83.2572347266881\n",
            "Acuracia de validacao na epoca:  286  e  86.1469387755102\n",
            "Acuracia de treinamento na epoca:  287  e  83.06338998621958\n",
            "Acuracia de validacao na epoca:  287  e  85.65306122448979\n",
            "Acuracia de treinamento na epoca:  288  e  83.14101975195223\n",
            "Acuracia de validacao na epoca:  288  e  82.4326530612245\n",
            "Acuracia de treinamento na epoca:  289  e  83.5383555351401\n",
            "Acuracia de validacao na epoca:  289  e  82.4938775510204\n",
            "Acuracia de treinamento na epoca:  290  e  83.48231511254019\n",
            "Acuracia de validacao na epoca:  290  e  89.5265306122449\n",
            "Acuracia de treinamento na epoca:  291  e  83.46394120349103\n",
            "Acuracia de validacao na epoca:  291  e  83.42857142857143\n",
            "Acuracia de treinamento na epoca:  292  e  83.28525493798806\n",
            "Acuracia de validacao na epoca:  292  e  84.85714285714285\n",
            "Acuracia de treinamento na epoca:  293  e  83.37436839687643\n",
            "Acuracia de validacao na epoca:  293  e  83.4326530612245\n",
            "Acuracia de treinamento na epoca:  294  e  83.59669269637115\n",
            "Acuracia de validacao na epoca:  294  e  81.47755102040817\n",
            "Acuracia de treinamento na epoca:  295  e  83.61276986678915\n",
            "Acuracia de validacao na epoca:  295  e  89.334693877551\n",
            "Acuracia de treinamento na epoca:  296  e  83.55948553054662\n",
            "Acuracia de validacao na epoca:  296  e  83.82448979591837\n",
            "Acuracia de treinamento na epoca:  297  e  83.0509875976114\n",
            "Acuracia de validacao na epoca:  297  e  86.13877551020407\n",
            "Acuracia de treinamento na epoca:  298  e  83.3321084060634\n",
            "Acuracia de validacao na epoca:  298  e  83.55510204081634\n",
            "Acuracia de treinamento na epoca:  299  e  83.5847496554892\n",
            "Acuracia de validacao na epoca:  299  e  85.13877551020407\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L856xl4UPKUO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "outputId": "6cce2d2b-f585-41b8-8f62-63452a8566ca"
      },
      "source": [
        "plt.plot([i+1 for i in range(num_epochs)],mean_valid_acc,label=\"Acuracia de validacao\")\n",
        "plt.plot([i+1 for i in range(num_epochs)],mean_train_acc,label=\"Acuracia de treinamento\")\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel(\"Epocas\")\n",
        "plt.ylabel(\"Acuracia em % \")\n",
        "plt.show()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nESillJfTRKCUUl5OE4FSSnm5/wXXpffu7fPVEwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nkLwwJWMfTp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "49996616-4412-49db-f413-a9d973f0c3aa"
      },
      "source": [
        "# Gerando palavras\n",
        "\n",
        "print(\"GERANDO TEXTO FALSO DE WILLIAM SHAKESPEARE: \\n________________________________________________________\")\n",
        "num_words=2000 # número de palavras que gostaríamos de gerar\n",
        "testacc=[]\n",
        "\n",
        "# Qual a primeira palavra palavra do texto (cold start)? Nesse caso será (The)\n",
        "\n",
        "input_word_index=return_index('The')\n",
        "print('The',end=\" \")\n",
        "X=torch.autograd.Variable(torch.LongTensor(np.array([input_word_index]).reshape(1,1)))\n",
        "hidden = model.init_hidden(1)\n",
        "\n",
        "for i in range(num_words):\n",
        "\n",
        "    if cuda.is_available():\n",
        "            X = X.cuda()\n",
        "           \n",
        "  \n",
        "  \n",
        "#     print(hidden.shape)\n",
        "    outputs,hidden=model(X,hidden)# outputs shape:  torch.Size([seq_lenght , batch_size, vocab_size])\n",
        "    outputs=outputs.view(vocab_size)\n",
        "    outExp=outputs.exp()\n",
        "#     print(outputs)\n",
        "    probablistic_output=(outExp)/(outExp.sum())# probablistic_output shape: torch.Size([vocab_size])\n",
        "#     probablistic_output=((outputs)/(0.4)).exp()\n",
        "\n",
        "    word_index = (torch.multinomial(probablistic_output, 1))\n",
        "    X=torch.autograd.Variable((word_index).reshape(1,1))\n",
        "    if(voc[word_index]==\"<end>\"):\n",
        "      print(\"\")\n",
        "    else:\n",
        "      print(voc[word_index],end=\" \")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GERANDO TEXTO FALSO DE WILLIAM SHAKESPEARE: \n",
            "________________________________________________________\n",
            "The alliance to thy brother Valentine; mine \n",
            "uncle Capulet, his wife and daughters; my fair niece \n",
            "Rosaline; Livia; Signior Valentio and his cousin \n",
            "Tybalt, Lucio and the lively Helena.' A fair \n",
            "assembly: whither should they come? \n",
            "\n",
            "Servant: \n",
            "Up. \n",
            "\n",
            "ROMEO: \n",
            "Whither? \n",
            "\n",
            "Servant: \n",
            "To supper; to our house. \n",
            "\n",
            "ROMEO: \n",
            "Whose house? \n",
            "\n",
            "Servant: \n",
            "My master's. \n",
            "\n",
            "ROMEO: \n",
            "Indeed, I should have ask'd you that before. \n",
            "\n",
            "Servant: \n",
            "Now I will tell thee, sir. \n",
            "\n",
            "ROMEO: \n",
            "In saying so, \n",
            "Her chariot is an empty hazel-nut \n",
            "Made by the joiner squirrel or old grub, \n",
            "Time out o' mind the fairies' coachmakers. \n",
            "And in this state she gallops night by night \n",
            "Through lovers' brains, and then they dream of love; \n",
            "O'er courtiers' knees, that dream on court'sies straight, \n",
            "O'er lawyers' fingers, who straight dream on fees, \n",
            "O'er ladies ' lips, who straight on kisses dream, \n",
            "Which oft the angry Mab with blisters plagues, \n",
            "Because their breaths with sweetmeats tainted are: \n",
            "Sometime she gallops o'er a courtier's nose, \n",
            "And then dreams he of smelling out a suit; \n",
            "And sometime comes she with a tithe-pig's tail \n",
            "Tickling a parson's nose as a' lies asleep, \n",
            "Then dreams, he of another benefice: \n",
            "Sometime she driveth o'er a soldier's neck, \n",
            "And then dreams he of cutting foreign throats, \n",
            "Of breaches, ambuscadoes, Spanish blades, \n",
            "Of healths five-fathom deep; and then anon \n",
            "Drums in his ear, at which he starts and wakes, \n",
            "But, my soul's worthy and this man's clout \n",
            "Steep'd in the faultless blood of pretty Rutland-- \n",
            "His curses, then from bitterness of soul \n",
            "Denounced against thee, are all fall'n upon thee; \n",
            "And God, not we, hath plagued thy bloody deed. \n",
            "\n",
            "QUEEN MARGARET: \n",
            "Urge it no better than a piece of marchpane; and, as thou lovest me, let \n",
            "him take the shapes of men, how can what say ye are \n",
            "\n",
            "MENENIUS: \n",
            "Look on them with the rest; for he was there \n",
            "To kindle their dry stubble; and their blaze \n",
            "Shall darken him for ever. \n",
            "\n",
            "BRUTUS: \n",
            "What's the matter? \n",
            "\n",
            "Messenger: \n",
            "You are sent for to the senate: \n",
            "A fearful army, led by Caius Marcius \n",
            "Associated with Aufidius, rages \n",
            "Upon our territories; and have already \n",
            "O'erborne their way, consumed with fire, and took \n",
            "What are to the account. \n",
            "\n",
            "Lieutenant: \n",
            "Sir, I beseech you, think you he'll carry Rome? \n",
            "\n",
            "AUFIDIUS: \n",
            "All places yield to him ere he sits down; \n",
            "And the nobility of Rome are his: \n",
            "The senators and patricians love him too: \n",
            "The tribunes there, attend my noble and \n",
            "A cold heads to the Tower; let him not speak. \n",
            "\n",
            "COMINIUS: \n",
            "Cut me to pieces, Volsces; men and lads, \n",
            "Stain all your edges on me. Boy! false hound! \n",
            "If you have writ your annals true, 'tis there, \n",
            "That, like an eagle in a dove-cote, I \n",
            "Flutter'd your Volscians in Corioli: \n",
            "Alone I did it. Boy! \n",
            "\n",
            "AUFIDIUS: \n",
            "Why, noble lords, \n",
            "Will you be put with honour such a fiery sun; \n",
            "Murdering impossibility, to make \n",
            "What cannot be, slight work. \n",
            "\n",
            "VOLUMNIA: \n",
            "Thou art my warrior; \n",
            "I holp to frame thee. What barm can your bisson \n",
            "conspectuities glean out of this character, if I be \n",
            "known well enough too? \n",
            "\n",
            "BRUTUS: \n",
            "Come, come, have you not? \n",
            "\n",
            "GLOUCESTER: \n",
            "No more words, we beseech you \n",
            "\n",
            "CORIOLANUS: \n",
            "Why should we do? \n",
            "\n",
            "MENENIUS: \n",
            "Return me that did end to lay me down. \n",
            "'Tis time to kindle, not to quench. \n",
            "\n",
            "First Senator: \n",
            "To unbuild the city and to lay all flat. \n",
            "\n",
            "SICINIUS: \n",
            "What is a city but the people? \n",
            "\n",
            "Citizens: \n",
            "True, \n",
            "The people are the city. \n",
            "\n",
            "BRUTUS: \n",
            "By your wish, the people's eyes:--his raising; \n",
            "Nothing but his report. \n",
            "\n",
            "Messenger: \n",
            "Yes, worthy sir, \n",
            "The slave's report is seconded; and more, \n",
            "More fearful, is deliver'd. \n",
            "\n",
            "SICINIUS: \n",
            "What more fearful? \n",
            "\n",
            "Messenger: \n",
            "It is spoke freely out of many mouths-- \n",
            "How probable I do not know--that Marcius, \n",
            "Join'd with Aufidius, leads a power 'gainst Rome, \n",
            "And vows revenge as spacious as between \n",
            "The young'st and oldest thing. \n",
            "\n",
            "SICINIUS: \n",
            "This is most likely! \n",
            "\n",
            "BRUTUS: \n",
            "Raised only, that the weaker sort may wish \n",
            "Good Marcius home again. \n",
            "\n",
            "SICINIUS: \n",
            "The very trick on't. \n",
            "\n",
            "MENENIUS: \n",
            "This thinks, nay, is all glad to break into his execution. \n",
            "See how he still together, who twin, as 'twere, in love \n",
            "Unseparable, shall within this hour, \n",
            "On a dissension of a doit, break out a long, \n",
            "But overmuch consumed his woes have friends, \n",
            "And in joy cannot tell him that we have begun to cry, \n",
            "Let them not cease, but with a din confused \n",
            "Enforce the present execution \n",
            "Of what we chance to sentence. \n",
            "\n",
            "AEdile: \n",
            "Very well. \n",
            "\n",
            "SICINIUS: \n",
            "Make them be strong and ready for this hint, \n",
            "When we shall hap to give 't them. \n",
            "\n",
            "BRUTUS: \n",
            "Go about it. \n",
            "Put him to choler straight: he hath been used \n",
            "Ever to conquer, and to have his worth \n",
            "Of contradiction: being once chafed, he cannot \n",
            "Be rein'd again to temperance; then he speaks \n",
            "What's in his heart; and that we know you there's \n",
            "No rather to be so contempt is not a friend \n",
            "To faith \n",
            "As you guess, Marcius, \n",
            "Their bands i' the vaward are the Antiates, \n",
            "Of their best trust; o'er them Aufidius, \n",
            "Their very heart of you. \n",
            "\n",
            "LARTIUS: \n",
            "He did, my lord. \n",
            "\n",
            "CORIOLANUS: \n",
            "How? what? \n",
            "\n",
            "SICINIUS: \n",
            "With gone; \n",
            "Well, I have some more \n",
            "Than this begot between two heads \n",
            "As what was to her charge, \n",
            "As I am by that long shall have him in, \n",
            "To lay him there, and the people, for \n",
            "He had, my lord; and that he could not \n",
            "That look upon his brow shame doth slay us speak. \n",
            "\n",
            "First Lord: \n",
            "There's no more made \n",
            "To whose eyes would pass on him. \n",
            "The noble knot he made. \n",
            "\n",
            "BRUTUS: \n",
            "I would to our own will. \n",
            "\n",
            "COMINIUS: \n",
            "God-den to you all, god-den to you all. \n",
            "\n",
            "First Citizen: \n",
            "Ourselves, our wives, and children, on our knees, \n",
            "Are bound to pray for you both. \n",
            "\n",
            "SICINIUS: \n",
            "Live, and thrive! \n",
            "\n",
            "SICINIUS: \n",
            "Go to, we see you? \n",
            "\n",
            "Lords: \n",
            "All world, in his wisdom to be bound? or thy mother, and \n",
            "My name is my repeal, as hasty to make it blown out of what thou hast \n",
            "often burst and now repaired with knots; one girth \n",
            "six time has a lonely dragon, that his fen \n",
            "Makes fear'd and talk'd of more than seen--your son \n",
            "Will or exceed the common or be caught \n",
            "With cautelous baits and practise. \n",
            "\n",
            "VOLUMNIA: \n",
            "My joy more, my husband yet thou wilt see thy name: \n",
            "I urged them one be made for some a \n",
            "noble lords, say you? \n",
            "\n",
            "BRAKENBURY: \n",
            "I prithee, speak not me; if you do do, \n",
            "I am in name of hate, and more soul \n",
            "Than fair than fear. O mother, \n",
            "A most inherent baseness. \n",
            "\n",
            "VOLUMNIA: \n",
            "At thy choice, then: \n",
            "To beg of thee, thy love, thou shalt be \n",
            "That thou hast worn it in a happy mother's mistress. \n",
            "\n",
            "AUFIDIUS: \n",
            "Ay, former time, in all the world, \n",
            "that, I hate the youngest for thee, sir, for me, \n",
            "I see my mother, a great danger. \n",
            "\n",
            "AUFIDIUS: I say, fair speech. \n",
            "I never saw thy Think'st thou I am an executioner? \n",
            "\n",
            "LADY ANNE: \n",
            "He is an quarrel since I will thee, forty more; whose issue \n",
            "With prayers all regarded \n",
            "As the most corse of that which he \n",
            "Hath made her down that kill'd his part of blame. \n",
            "Let's not that farewell, good fellow. \n",
            "\n",
            "GLOUCESTER: \n",
            "Dear earth, and, now have war, yet out of length, \n",
            "That reacheth from the restful English court \n",
            "As far as Calais, to mine uncle's head?' \n",
            "Amongst much other talk, that very himself, \n",
            "I country's thee with a most cause to seek me forth with me: \n",
            "Her might be kept it is a pity, so, if it must \n",
            "Be not so much to be put in either \n",
            "So soon as rich as you so much your breast, \n",
            "In dear employment: therefore hence, be gone: \n",
            "But if thou, jealous, dost return to thee, \n",
            "In just and yours, fair fortune of \n",
            "The book of heaven in his growth. \n",
            "\n",
            "YORK: \n",
            "Then would we were to slander it, in his mistress' circle \n",
            "Of some strange breath as I am struck me of a shame, \n",
            "But wilt thou prove a poison now, \n",
            "Whose bright out-shining beams thy cloudy wrath \n",
            "Hath in eternal darkness folded up. \n",
            "Your aery buildeth in our aery's nest. \n",
            "O God, that seest it, here! \n",
            "I do most country's riches but a foot \n",
            "Will would not fall your cursed by \n",
            "Which thou shalt then be much against her daughter, \n",
            "And so much more than much in this new world, \n",
            "Even in my hour lord and fell things with these rights, \n",
            "Advance your standards, draw your willing swords. \n",
            "For me, in God's blessings I have done you God, \n",
            "So did not well by my just ordinance, \n",
            "Ere from this war thou turn a conqueror, \n",
            "Or I with grief and extreme age shall perish \n",
            "And never look upon thy face again. \n",
            "Therefore take with you my heart. \n",
            "\n",
            "THOMAS MOWBRAY: \n",
            "Then let this counterfeit one the denier. Go by, Jeronimy: go to thy legs. \n",
            "\n",
            "ROMEO: \n",
            "A torch for me: let wantons light of heart \n",
            "Tickle the senseless rushes with their heels, \n",
            "For I am proverb'd with a grandsire phrase; \n",
            "I'll be a candle-holder, and look on. \n",
            "The game was ne'er so fair, and I am a man, \n",
            "Those being is it in hare that is hoar \n",
            "Is our late behind his gest \n",
            "Prefix'd for's parting: yet, good deed, Leontes, \n",
            "I love thee not a joyful time of day! \n",
            "\n",
            "QUEEN ELIZABETH: \n",
            "As much to you, good sister! Whither away? \n",
            "\n",
            "LADY ANNE: \n",
            "No farther than the Tower; and, as I guess, \n",
            "To make a bloody supper in the Tower. \n",
            "\n",
            "JULIET: \n",
            "Is that good friends, let's take this thou, man, \n",
            "I'll leave our gentle brother with her good, \n",
            "In that thou seest thy wretched brother die, \n",
            "Who was the model of thy father's life. \n",
            "Call it not patience, Gaunt; it is despair: \n",
            "In suffering thus thy brother to be slaughter'd, \n",
            "Thou showest the naked pathway to thy life, \n",
            "Teaching stern murder how to butcher thee: \n",
            "That which in mean manners still thou art. \n",
            "\n",
            "QUEEN ELIZABETH: \n",
            "To dear and thy good love about, \n",
            "And thou but doubt not reason and tell her "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLR_S72lPhuW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}